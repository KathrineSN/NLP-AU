{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classroom 6 - Training a Named Entity Recognition Model with a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classroom today is primarily geared towards preparing you for Assignment 4 which you'll be working on after today. The notebook is split into three main parts to get you thinking. You should work through these sections in groups together in class. \n",
    "\n",
    "If you have any questions or things you don't understand, make a note of them so you can remember to ask - or, even better, post them to Slack!\n",
    "\n",
    "If you get through everything here, make a start on the assignment. If you don't, dont' worry about it - but I suggest you finish all of the exercises here before starting the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A very short intro to NER\n",
    "Named entity recognition (NER) also known as named entity extraction, and entity identification is the task of tagging an entity is the task of extracting which seeks to extract named entities from unstructured text into predefined categories such as names, medical codes, quantities or similar.\n",
    "\n",
    "The most common variant is the [CoNLL-20003](https://www.clips.uantwerpen.be/conll2003/ner/) format which uses the categories, person (PER), organization (ORG) location (LOC) and miscellaneous (MISC), which for example denote cases such nationalies. For example:\n",
    "\n",
    "*Hello my name is $Ross_{PER}$ I live in $Aarhus_{LOC}$ and work at $AU_{ORG}$.*\n",
    "\n",
    "For example, let's see how this works with ```spaCy```. NB: you might need to remember to install a ```spaCy``` model:\n",
    "\n",
    "```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /usr/lib/python3/dist-packages (20.3.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 4.6 MB/s eta 0:00:00\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 15.9 MB/s eta 0:00:00\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 441.9/441.9 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp39-cp39-manylinux1_x86_64.whl (890.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 890.2/890.2 MB 4.5 MB/s eta 0:00:00\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.0/24.0 MB 27.9 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (815 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 815.9/815.9 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.8/42.8 kB 101.4 kB/s eta 0:00:00\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy) (1.23.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.7/128.7 kB 592.1 kB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 490.7/490.7 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (52.0.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/13.2 MB 29.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.6/181.6 kB 604.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in /home/coder/.local/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/coder/.local/lib/python3.9/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/coder/.local/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/coder/.local/lib/python3.9/site-packages (from datasets) (8.0.0)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.5/163.5 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/coder/.local/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.0/212.0 kB 966.0 kB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.9/132.9 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in /home/coder/.local/lib/python3.9/site-packages (from datasets) (1.4.3)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 8.2 MB/s eta 0:00:00\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 13.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in /home/coder/.local/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 44.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.6/58.6 kB 158.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/coder/.local/lib/python3.9/site-packages (from gensim) (1.8.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/coder/.local/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.6/58.6 kB 218.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 48.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/coder/.local/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 kB 12.2 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/coder/.local/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/coder/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/coder/.local/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.3/132.3 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/coder/.local/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/coder/.local/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/coder/.local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: wasabi, cymem, yarl, xxhash, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, pip, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, murmurhash, multiprocess, langcodes, frozenlist, filelock, catalogue, blis, async-timeout, srsly, responses, preshed, pathy, nvidia-cudnn-cu11, huggingface-hub, gensim, aiosignal, torch, confection, aiohttp, thinc, spacy, datasets\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 datasets-2.6.1 filelock-3.8.0 frozenlist-1.3.1 gensim-4.2.0 huggingface-hub-0.10.1 langcodes-3.3.0 multiprocess-0.70.13 murmurhash-1.0.9 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pathy-0.6.2 pip-22.3 preshed-3.0.8 pydantic-1.10.2 responses-0.18.0 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 torch-1.13.0 typer-0.4.2 wasabi-0.10.1 xxhash-3.1.0 yarl-1.8.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 11.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/coder/.local/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (52.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/coder/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/coder/.local/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/coder/.local/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/coder/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/coder/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/coder/.local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/coder/.local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/coder/.local/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/coder/.local/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip install --upgrade pip spacy datasets torch gensim')\n",
    "os.system('python3 -m spacy download en_core_web_sm')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello my name is Ross. I live in Denmark and work at Aarhus University, I am Scottish and today is Friday 27th.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello my name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ross\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". I live in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Denmark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and work at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aarhus University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", I am \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Scottish\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    today\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Friday 27th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging standards\n",
    "There exist different tag standards for NER. The most used one is the BIO-format which frames the task as token classification denoting inside, outside and beginning of a token. \n",
    "\n",
    "Words marked with *O* are not a named entity. Words with NER tags which start with *B-\\** indicate the start of a multiword entity (i.e. *B-ORG* for the *Aarhus* in *Aarhus University*), while *I-\\** indicate the continuation of a token (e.g. University).\n",
    "\n",
    "    B = Beginning\n",
    "    I = Inside\n",
    "    O = Outside\n",
    "\n",
    "<details>\n",
    "<summary>Q: What other formats and standards are available? What kinds of entities do they make it possible to tag?</summary>\n",
    "<br>\n",
    "You can see more examples on the spaCy documentation for their [different models(https://spacy.io/models/en)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello O\n",
      "my O\n",
      "name O\n",
      "is O\n",
      "Ross B-PERSON\n",
      ". O\n",
      "I O\n",
      "live O\n",
      "in O\n",
      "Denmark B-GPE\n",
      "and O\n",
      "work O\n",
      "at O\n",
      "Aarhus B-ORG\n",
      "University I-ORG\n",
      ", O\n",
      "I O\n",
      "am O\n",
      "Scottish B-NORP\n",
      "and O\n",
      "today B-DATE\n",
      "is O\n",
      "Friday B-DATE\n",
      "27th I-DATE\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    if t.ent_type:\n",
    "        print(t, f\"{t.ent_iob_}-{t.ent_type_}\")\n",
    "    else:\n",
    "        print(t, t.ent_iob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some challenges with NER\n",
    "While NER is currently framed as above this formulating does contain some limitations. \n",
    "\n",
    "For instance the entity Aarhus University really refers to both the location Aarhus, the University within Aarhus, thus nested NER (N-NER) argues that it would be more correct to tag it in a nested fashion as \\[\\[$Aarhus_{LOC}$\\] $University$\\]$_{ORG}$ (Plank, 2020). \n",
    "\n",
    "Other task also include named entity linking. Which is the task of linking an entity to e.g. a wikipedia entry, thus you have to both know that it is indeed an entity and which entity it is (if it is indeed a defined entity).\n",
    "\n",
    "In this assignment, we'll be using Bi-LSTMs to train an NER model on a predifined data set which uses IOB tags of the kind we outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training in batches\n",
    "\n",
    "When you trained your document classifier for the last assignment, you probably noticed that the neural network was quite brittle. Small changes in the hyperparameters could cause massive changes in performance. Likewise, you probably noticed that they tend to substantially overfit the training data and underperform on the validation and test data.\n",
    "\n",
    "One way we can get around this is by processing the data in smaller chunks known as *batches*. \n",
    "\n",
    "<details>\n",
    "<summary>Q: Why might it be a good idea to train on batches, rather than the whole dataset?</summary>\n",
    "<br>\n",
    "These batches are usually small (something like 32 instances at a time) but they have couple of important effects on training:\n",
    "\n",
    "- Batches can be processed in parallel, rather the sequentially. This can result in substantial speed up from computational perspective\n",
    "- Similarly, smaller batch sizes make it easier to fit training data into memory\n",
    "- Lastly,  smaller batch sizes are noisy, meaning that they have a regularizing effect and thus lead to less overfitting.\n",
    "\n",
    "In this assignment, we're going to be using batches of data to train our NER model. To do that, we first have to prepare our batches for training. You can read more about batching in [this blog post](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/).\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When having large amounts of data it is a good idea to perform batch processing as loading in smaller parts of the data saves memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this allows us to look one step up in the directory\n",
    "# for importing custom modules from src\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.util import batch\n",
    "from src.LSTM import RNN\n",
    "from src.embedding import gensim_to_torch_embedding\n",
    "\n",
    "# numpy and pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# loading data and embeddings\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the datset using the ```load_dataset()``` function we've already seen. Here we take only the training data.\n",
    "\n",
    "When you've downloaded the dataset, you're welcome to save a local copy so that we don't need to constantly download it again everytime the code runs.\n",
    "\n",
    "Q: What do the ```train.features``` values refer to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As far as I understand different taggings of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conllpp (/home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29201741c90a44baa26eb0d130a8f4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATASET\n",
    "dataset = load_dataset(\"conllpp\")\n",
    "train = dataset[\"train\"]\n",
    "\n",
    "# inspect the dataset\n",
    "train[\"tokens\"][:1]\n",
    "train[\"ner_tags\"][:1]\n",
    "\n",
    "# get number of classes\n",
    "num_classes = train.features[\"ner_tags\"].feature.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']]\n",
      "[[3, 0, 7, 0, 0, 0, 7, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train[\"tokens\"][:1]\n",
    ")\n",
    "print(train[\"ner_tags\"][:1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use ```gensim``` to get some pretrained word embeddings for the input layer to the model. \n",
    "\n",
    "In this example, we're going to use a GloVe model pretrained on Wikipedia, with 50 dimensions.\n",
    "\n",
    "I've provided a helper function to take the ```gensim``` embeddings and prepare them for ```pytorch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[[-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011\n",
      "   0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802\n",
      "   0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912\n",
      "   0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227\n",
      "   0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107\n",
      "  -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544\n",
      "  -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303\n",
      "  -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408\n",
      "   0.00653003  0.01685157]]\n",
      "(1, 50)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "(1, 50)\n",
      "embedding\n",
      "[[ 0.41800001  0.24968    -0.41242    ... -0.18411    -0.11514\n",
      "  -0.78580999]\n",
      " [ 0.013441    0.23682    -0.16899    ... -0.56656998  0.044691\n",
      "   0.30392   ]\n",
      " [ 0.15164     0.30177    -0.16763    ... -0.35652     0.016413\n",
      "   0.10216   ]\n",
      " ...\n",
      " [ 0.072617   -0.51393002  0.47279999 ... -0.18907    -0.59021002\n",
      "   0.55558997]\n",
      " [-0.12920076 -0.28866628 -0.01224866 ...  0.10069408  0.00653003\n",
      "   0.01685157]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "(400002, 50)\n",
      "weight\n",
      "tensor([[ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
      "        [ 0.0134,  0.2368, -0.1690,  ..., -0.5666,  0.0447,  0.3039],\n",
      "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
      "        ...,\n",
      "        [ 0.0726, -0.5139,  0.4728,  ..., -0.1891, -0.5902,  0.5556],\n",
      "        [-0.1292, -0.2887, -0.0122,  ...,  0.1007,  0.0065,  0.0169],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "torch.Size([400002, 50])\n",
      "emb layer\n",
      "Embedding(400002, 50, padding_idx=400001)\n"
     ]
    }
   ],
   "source": [
    "from src.embedding import gensim_to_torch_embedding\n",
    "import importlib\n",
    "\n",
    "# CONVERTING EMBEDDINGS\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# convert gensim word embedding to torch word embedding\n",
    "embedding_layer, vocab = gensim_to_torch_embedding(model)\n",
    "# this word embedding has 50 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(400002, 50, padding_idx=400001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer # the embedding layer basically contains the vector re-presentation for all words in the vocabulary along with a vector for unknown and pad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a batch\n",
    "\n",
    "The first thing we want to do is to shuffle our dataset before training. \n",
    "\n",
    "Why might it be a good idea to shuffle the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To ensure generalisability by avoiding bias in the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438.78125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle dataset\n",
    "shuffled_train = dataset[\"train\"].shuffle(seed=1)\n",
    "len(shuffled_train[\"tokens\"])/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to bundle the shuffled training data into smaller batches of predefined size. I've written a small utility function here to help. \n",
    "\n",
    "<details>\n",
    "<summary>Q: Can you explain how the ```batch()``` function works?</summary>\n",
    "<br>\n",
    " Hint: Check out [this link](https://realpython.com/introduction-to-python-generators/).\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batches_tokens = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "batches_tags = batch(shuffled_train[\"ner_tags\"], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use the ```tokens_to_idx()``` function below on our batches.\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is this function doing? Why is it doing it?</summary>\n",
    "<br>\n",
    "We're making everything lowercase and adding a new, arbitrary token called <UNK> to the vocabulary. This <UNK> means \"unknown\" and is used to replace out-of-vocabulary tokens in the data - i.e. tokens that don't appear in the vocabulary of the pretrained word embeddings.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idx(tokens, vocab=model.key_to_index):\n",
    "    \"\"\" This function finds a the index of a specific token in the vocabulary\n",
    "    Args:\n",
    "        tokens: the generator object of tokenized sentences to loop through\n",
    "        vocab: vocabulary dictionary\n",
    "    \"\"\"\n",
    "    return [vocab.get(t.lower(), vocab[\"UNK\"]) for t in tokens] # the .get method, finds the idx of the word in the dictionary. The words are lower case because of the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check below that everything is working as expected by testing it on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch at 0x7f12cb483660>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "['shares', 'outstanding']\n",
      "(['shares', 'outstanding'], ['Oldham', '22', '9', '1', '12', '473', '681', '19'], ['Goldman', 'Sachs', '&', 'Co', 'Wertpapier', 'GmbH', 'has', 'issued', 'a', 'total', 'of', 'five', 'million', 'American-style', 'call', 'warrants', ',', 'on', 'Continental', 'AG', ',', 'lead', 'manager', 'Goldman', 'Sachs', '&', 'Co', 'said', '.'], ['CINCINNATI', '64', '63', '.504', '3', '1/2'], ['MELBOURNE', '1996-08-23'], ['--', 'FLNC', 'Corsican', 'nationalist', 'movement', 'announces', 'end', 'of', 'truce', 'after', 'last', 'night', \"'s\", 'attacks', '.'], ['4-6', '7-6', '(', '7-4', ')'], ['Dealers', 'said', 'that', 'the', 'volume', 'of', 'longer-term', 'government', 'paper', 'declined', 'due', 'to', 'market', 'nervousness', '.'], ['MONTREAL', 'AT', 'SAN', 'FRANCISCO'], ['\"', 'I', 'would', 'love', 'to', 'speak', 'about', 'everything', ',', '\"', 'said', 'Simpson', ',', 'who', 'vowed', 'after', 'his', 'acquittal', 'to', 'find', 'the', 'killers', 'and', 'offered', 'a', 'substantial', 'reward', '.'], ['The', 'back-to-back', 'wins', 'vault', 'Ferrigato', 'from', 'sixth', 'to', 'second', 'in', 'the', 'overall', 'World', 'Cup', 'rankings', 'with', '112', 'points', 'but', 'Museeuw', 'continues', 'to', 'hold', 'a', 'commanding', 'lead', 'with', '162', 'points', 'after', 'eight', 'of', 'the', '11', 'rounds', '.'], ['Voters', 'will', 'be', 'choosing', 'a', 'three-member', 'presidency', 'and', 'a', 'parliament', 'to', 'rule', 'over', 'a', 'loose', 'union', 'of', 'Bosnia', ',', 'comprised', 'of', 'a', 'Serb', 'republic', 'and', 'a', 'Moslem-Croat', 'federation', '.'], ['*'], ['winning', 'percentage', ',', 'games', 'behind', 'first', 'place', ')'], ['This', 'compared', 'with', 'midweek', 'levels', ',', 'before', 'the', 'welter', 'of', 'interest', 'rate', 'cuts', ',', 'of', '18.50', 'for', 'September', ',', '20.00', 'for', 'December', ',', '22.00', 'for', 'March', 'and', '23.5', 'for', 'June', ',', 'he', 'said', '.'], ['The', 'letter', 'made', 'reference', 'to', 'massacres', 'of', 'landless', 'peasants', 'in', 'August', '1995', 'and', 'April', '1996', ',', 'which', 'claimed', 'the', 'lives', 'of', '27', 'landless', 'peasants', '.'], ['The', 'suit', 'was', 'filed', 'on', 'Wednesday', 'in', 'federal', 'court', 'in', 'Manhattan', 'and', 'is', 'another', 'legal', 'skirmish', 'over', 'what', 'constitutes', 'a', '\"', 'broadcast', '\"', 'in', 'the', 'computer', 'age', '.'], ['Marc', 'Dutroux', ',', 'the', 'chief', 'accused', 'in', 'a', 'Belgian', 'child', 'murder', 'and', 'sex', 'abuse', 'scandal', ',', 'is', 'suspected', 'of', 'murdering', 'a', 'young', 'Slovak', 'woman', ',', 'the', 'Slovak', 'office', 'of', 'Interpol', 'said', 'on', 'Wednesday', '.'], ['Year', 'to', 'June', '30', '.'], ['9.', 'Michael', 'Andersson', '(', 'Sweden', ')', 'Telekom', '54'], ['drawn', ',', 'lost', ',', 'goals', 'for', ',', 'against', ',', 'points', ')', ':'], ['Scorers', ':', 'Petr', 'Gunda', '(', '1st', 'and', '26th', ')', ',', 'Lumir', 'Mistr', '(', '19th', ')', ','], ['ALMERE', ',', 'Netherlands', '1996-08-28'], ['The', 'shot', 'brought', 'home', 'Ivan', 'Rodriguez', ',', 'who', 'had', 'his', 'second', 'double', 'of', 'the', 'game', ',', 'giving', 'him', '42', 'this', 'season', ',', '41', 'as', 'a', 'catcher', '.'], ['Fall', 'of', 'wickets', ':', '1-57', '2-98', '3-146', '4-200', '5-220', '.'], ['\"', 'Both', 'countries', \"'\", 'governments', 'were', 'formed', 'recently', '.'], ['\"', 'When', 'the', '(', 'profit', ')', 'figures', 'will', 'bounce', 'back', 'up', 'again', 'is', 'just', 'a', 'function', 'of', 'markets', 'recovering', 'just', 'a', 'fraction', ',', '\"', 'he', 'added', '.'], ['CHICAGO', '1996-08-27'], ['7.', 'Michel', 'Zanoli', '(', 'Netherlands', ')', 'MX', 'Onda'], ['15,000', '.'], ['CRICKET', '-', 'INDIA', 'BANS', 'SIDHU', 'FOR', '50', 'DAYS', '.'], ['2.', 'Ashia', 'Hansen', '(', 'Britain', ')', '14.78'])\n"
     ]
    }
   ],
   "source": [
    "# sample using only the first batch\n",
    "batch_tokens = next(batches_tokens)\n",
    "batch_tags = next(batches_tags)\n",
    "batch_tok_idx = [tokens_to_idx(sent) for sent in batch_tokens]\n",
    "print(len(batch_tokens))\n",
    "print(batch_tokens[0])\n",
    "print(batch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(batches_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Loops through until the data set is empty \n",
    "c = 0\n",
    "for b in batches_tokens:\n",
    "    c += 1\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example structure\n",
    "epochs = 5 \n",
    "count0 = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for e in range(epochs): # looping through epochs\n",
    "    count0 += 1\n",
    "    for b in batches_tokens: # looping through batches\n",
    "        batch_tokens = b\n",
    "        print(b)\n",
    "        count1 += 1\n",
    "        for tokens in batch_tokens:\n",
    "            print(tokens)\n",
    "            count2 += 1\n",
    "print(count0)\n",
    "print(count1)\n",
    "print(count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with document classification, our model needs to take input sequences of a fixed length. To get around this we do a couple of different steps.\n",
    "\n",
    "- Find the length of the longest sequence in the batch\n",
    "- Pad shorter sequences to the max length using an arbitrary token like <PAD>\n",
    "- Give the <PAD> token a new label ```-1``` to differentiate it from the other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute length of longest sentence in batch\n",
    "batch_max_len = max([len(s) for s in batch_tok_idx])\n",
    "batch_max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continue from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Can you figure out the logic of what is happening in the next two cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len)) # creating an array with a row for each sentence in the batch and a column for each word\n",
    "batch_labels = -1 * np.ones((batch_size, batch_max_len)) # Creating a corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[400001., 400001., 400001., ..., 400001., 400001., 400001.],\n",
       "       [400001., 400001., 400001., ..., 400001., 400001., 400001.],\n",
       "       [400001., 400001., 400001., ..., 400001., 400001., 400001.],\n",
       "       ...,\n",
       "       [400001., 400001., 400001., ..., 400001., 400001., 400001.],\n",
       "       [400001., 400001., 400001., ..., 400001., 400001., 400001.],\n",
       "       [400001., 400001., 400001., ..., 400001., 400001., 400001.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., ..., -1., -1., -1.],\n",
       "       [-1., -1., -1., ..., -1., -1., -1.],\n",
       "       [-1., -1., -1., ..., -1., -1., -1.],\n",
       "       ...,\n",
       "       [-1., -1., -1., ..., -1., -1., -1.],\n",
       "       [-1., -1., -1., ..., -1., -1., -1.],\n",
       "       [-1., -1., -1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data to the numpy array\n",
    "for i in range(batch_size): # looping through each sentence in the batch\n",
    "    tok_idx = batch_tok_idx[i] #assigning the index to each word\n",
    "    tags = batch_tags[i]\n",
    "    size = len(tok_idx) # number of words in the sentence\n",
    "\n",
    "    batch_input[i][:size] = tok_idx # fill in the indexes as inputs\n",
    "    batch_labels[i][:size] = tags # fill in the tags as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., -1., ..., -1., -1., -1.],\n",
       "       [ 3.,  0.,  0., ..., -1., -1., -1.],\n",
       "       [ 3.,  4.,  4., ..., -1., -1., -1.],\n",
       "       ...,\n",
       "       [ 0.,  0., -1., ..., -1., -1., -1.],\n",
       "       [ 0.,  0.,  5., ..., -1., -1., -1.],\n",
       "       [ 0.,  1.,  2., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels # NB! the dataset is constructed such that the labels are numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to conver the arrays into ```pytorch``` tensors, ready for the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all data are indices, we convert them to torch LongTensors (integers)\n",
    "batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(\n",
    "    batch_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 36])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now batched and processed, we want to run it through our RNN the same way as when we trained a classifier. Note that this cell is incomplete and won't yet run; that's part of the assignment!\n",
    "\n",
    "Q: Why is ```output_dim = num_classes + 1```?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[INFO:] Training classifier...\n",
      "epoch no: 1\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch no: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/work/NLP-AU/nbs/classroom_06.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://app-242558-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000046vscode-remote?line=83'>84</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://app-242558-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000046vscode-remote?line=85'>86</a>\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://app-242558-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000046vscode-remote?line=86'>87</a>\u001b[0m     batch_tokens_val \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(batches_tokens_val)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-242558-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000046vscode-remote?line=87'>88</a>\u001b[0m     batch_tags_val \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(batches_tags_val)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-242558-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000046vscode-remote?line=88'>89</a>\u001b[0m     batch_tok_idx_val \u001b[39m=\u001b[39m [tokens_to_idx(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m batch_tokens_val]\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.LSTM import RNN\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# CREATE MODEL\n",
    "model = RNN(\n",
    "    embedding_layer=embedding_layer, output_dim=num_classes + 1, hidden_dim_size=256\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "best_accuracy = 0.0\n",
    "print(\"[INFO:] Training classifier...\")\n",
    "\n",
    "epoch_train_loss_history = []\n",
    "epoch_val_loss_history = []\n",
    "running_accuracy = 0.0 \n",
    "running_val_loss = 0.0 \n",
    "total = 0 \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch no: ' + str(epoch + 1))\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    accuracies = []\n",
    "\n",
    "    # shuffle the data\n",
    "    shuffled_train = dataset[\"train\"].shuffle(seed = 1)\n",
    "    shuffled_val = dataset[\"validation\"].shuffle(seed = 1)\n",
    "        \n",
    "    # generate batches\n",
    "    batches_tokens_train = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "    batches_tags_train = batch(shuffled_train[\"ner_tags\"], batch_size)\n",
    "    batches_tokens_val = batch(shuffled_val[\"tokens\"], batch_size)\n",
    "    batches_tags_val = batch(shuffled_val[\"ner_tags\"], batch_size)\n",
    "\n",
    "    num_batches_train = len(shuffled_train[\"tokens\"])//batch_size\n",
    "    num_batches_val = len(shuffled_train[\"tokens\"])//batch_size\n",
    "\n",
    "    # iterate through the batches\n",
    "    for batch_number in range(num_batches_train):\n",
    "        print('batch no: ' + str(batch_number))\n",
    "    \n",
    "        batch_tokens_train = next(batches_tokens_train)\n",
    "        batch_tags_train = next(batches_tags_train)\n",
    "        batch_tok_idx_train = [tokens_to_idx(sent) for sent in batch_tokens_train]\n",
    "\n",
    "        # compute length of the sentence \n",
    "        batch_max_len_train = max([len(s) for s in batch_tok_idx_train])\n",
    "\n",
    "        # create padding\n",
    "        batch_input_train = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len_train)) # creating an array with a row for each sentence in the batch and a column for each word\n",
    "        batch_labels_train = -1 * np.ones((batch_size, batch_max_len_train)) # Creating a corresponding label\n",
    "\n",
    "        for i in range(batch_size): # looping through each sentence in the batch\n",
    "            tok_idx_train = batch_tok_idx_train[i] #assigning the index to each word\n",
    "            tags_train = batch_tags_train[i]\n",
    "            size = len(tok_idx_train) # number of words in the sentence\n",
    "            #print(len(tok_idx_train))\n",
    "            #print(len(tags_train))\n",
    "\n",
    "            batch_input_train[i][:size] = tok_idx_train # fill in the indexes as inputs\n",
    "            batch_labels_train[i][:size] = tags_train # fill in the tags as labels\n",
    "        \n",
    "        # convert to tensors\n",
    "        batch_input_train, batch_labels_train = torch.LongTensor(batch_input_train), torch.LongTensor(batch_labels_train)\n",
    "\n",
    "        # forward\n",
    "        X_train = batch_input_train\n",
    "        y_train = batch_labels_train\n",
    "        y_train_hat = model(X_train)\n",
    "\n",
    "        # backward\n",
    "        train_loss = model.loss_fn(outputs=y_train_hat, labels=y_train)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        # backpropagation\n",
    "        train_loss.backward()\n",
    "\n",
    "        # take step, reset\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            batch_tokens_val = next(batches_tokens_val)\n",
    "            batch_tags_val = next(batches_tags_val)\n",
    "            batch_tok_idx_val = [tokens_to_idx(sent) for sent in batch_tokens_val]\n",
    "\n",
    "            # compute length of the sentence \n",
    "            batch_max_len_val = max([len(s) for s in batch_tok_idx_val])\n",
    "\n",
    "            # create padding\n",
    "            batch_input_val = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len_val)) # creating an array with a row for each sentence in the batch and a column for each word\n",
    "            batch_labels_val = -1 * np.ones((batch_size, batch_max_len_val)) # Creating a corresponding label\n",
    "\n",
    "            for i in range(batch_size): # looping through each sentence in the batch\n",
    "                try:\n",
    "                    tok_idx_val = batch_tok_idx_val[i] #assigning the index to each word\n",
    "                    tags_val = batch_tags_val[i]\n",
    "                    size = len(tok_idx_val) # number of words in the sentence\n",
    "\n",
    "                    batch_input_val[i][:size] = tok_idx_val # fill in the indexes as inputs\n",
    "                    batch_labels_val[i][:size] = tags_val # fill in the tags as labels\n",
    "                except IndexError:\n",
    "                    break\n",
    "                    \n",
    "            # convert to tensors\n",
    "            batch_input_val, batch_labels_val = torch.LongTensor(batch_input_val), torch.LongTensor(batch_labels_val)\n",
    "\n",
    "            # forward\n",
    "            X_val = batch_input_val\n",
    "            y_val = batch_labels_val\n",
    "            y_val_hat = model(X_val)\n",
    "\n",
    "            # backward\n",
    "            y_val_hat = model(X_val) \n",
    "            val_loss = model.loss_fn(outputs=y_val_hat, labels=y_val)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            # the label with the highest value will be our prediction \n",
    "            _, predicted = torch.max(y_val_hat, 1) \n",
    "            running_val_loss += val_loss.item()  \n",
    "            total += y_val.size(0) \n",
    "            predicted = predicted.reshape(y_val.shape[0],y_val.shape[1])\n",
    "            running_accuracy += (predicted == y_val).sum().item()\n",
    "\n",
    "            # calculate validation loss value \n",
    "            val_loss_value = running_val_loss/len(y_val) \n",
    "                        \n",
    "            # calculate accuracy as the number of correct predictions in the validation batch divided by the total number of predictions done.  \n",
    "            accuracy = (100 * running_accuracy / total) \n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        epoch_acc = np.sum(np.array([val for val in accuracies]))/batch_size\n",
    "        val_loss = np.sum(np.array([val.item() for val in val_loss_history]))/batch_size\n",
    "        epoch_val_loss_history.append(val_loss)  \n",
    "\n",
    "        # save the model if the accuracy is the best \n",
    "        if epoch_acc > best_accuracy: \n",
    "            #saveModel(model) \n",
    "            best_accuracy = epoch_acc\n",
    "\n",
    "    \n",
    "    epoch_train_loss_history.append(np.sum(np.array([val.item() for val in train_loss_history]))/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[INFO:] Training classifier...\n",
      "epoch no: 1\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch no: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch: 2, train_loss = 0.1250\n",
      "<class 'generator'>\n",
      "0\n",
      "<class 'generator'>\n",
      "1\n",
      "<class 'generator'>\n",
      "2\n",
      "<class 'generator'>\n",
      "3\n",
      "<class 'generator'>\n",
      "4\n",
      "<class 'generator'>\n",
      "5\n",
      "<class 'generator'>\n",
      "6\n",
      "<class 'generator'>\n",
      "7\n",
      "<class 'generator'>\n",
      "8\n",
      "<class 'generator'>\n",
      "9\n",
      "<class 'generator'>\n",
      "10\n",
      "<class 'generator'>\n",
      "11\n",
      "<class 'generator'>\n",
      "12\n",
      "<class 'generator'>\n",
      "13\n",
      "<class 'generator'>\n",
      "14\n",
      "<class 'generator'>\n",
      "15\n",
      "<class 'generator'>\n",
      "16\n",
      "<class 'generator'>\n",
      "17\n",
      "<class 'generator'>\n",
      "18\n",
      "<class 'generator'>\n",
      "19\n",
      "<class 'generator'>\n",
      "20\n",
      "<class 'generator'>\n",
      "21\n",
      "<class 'generator'>\n",
      "22\n",
      "<class 'generator'>\n",
      "23\n",
      "<class 'generator'>\n",
      "24\n",
      "<class 'generator'>\n",
      "25\n",
      "<class 'generator'>\n",
      "26\n",
      "<class 'generator'>\n",
      "27\n",
      "<class 'generator'>\n",
      "28\n",
      "<class 'generator'>\n",
      "29\n",
      "<class 'generator'>\n",
      "30\n",
      "<class 'generator'>\n",
      "31\n",
      "<class 'generator'>\n",
      "32\n",
      "<class 'generator'>\n",
      "33\n",
      "<class 'generator'>\n",
      "34\n",
      "<class 'generator'>\n",
      "35\n",
      "<class 'generator'>\n",
      "36\n",
      "<class 'generator'>\n",
      "37\n",
      "<class 'generator'>\n",
      "38\n",
      "<class 'generator'>\n",
      "39\n",
      "<class 'generator'>\n",
      "40\n",
      "<class 'generator'>\n",
      "41\n",
      "<class 'generator'>\n",
      "42\n",
      "<class 'generator'>\n",
      "43\n",
      "<class 'generator'>\n",
      "44\n",
      "<class 'generator'>\n",
      "45\n",
      "<class 'generator'>\n",
      "46\n",
      "<class 'generator'>\n",
      "47\n",
      "<class 'generator'>\n",
      "48\n",
      "<class 'generator'>\n",
      "49\n",
      "<class 'generator'>\n",
      "50\n",
      "<class 'generator'>\n",
      "51\n",
      "<class 'generator'>\n",
      "52\n",
      "<class 'generator'>\n",
      "53\n",
      "<class 'generator'>\n",
      "54\n",
      "<class 'generator'>\n",
      "55\n",
      "<class 'generator'>\n",
      "56\n",
      "<class 'generator'>\n",
      "57\n",
      "<class 'generator'>\n",
      "58\n",
      "<class 'generator'>\n",
      "59\n",
      "<class 'generator'>\n",
      "60\n",
      "<class 'generator'>\n",
      "61\n",
      "<class 'generator'>\n",
      "62\n",
      "<class 'generator'>\n",
      "63\n",
      "<class 'generator'>\n",
      "64\n",
      "<class 'generator'>\n",
      "65\n",
      "<class 'generator'>\n",
      "66\n",
      "<class 'generator'>\n",
      "67\n",
      "<class 'generator'>\n",
      "68\n",
      "<class 'generator'>\n",
      "69\n",
      "<class 'generator'>\n",
      "70\n",
      "<class 'generator'>\n",
      "71\n",
      "<class 'generator'>\n",
      "72\n",
      "<class 'generator'>\n",
      "73\n",
      "<class 'generator'>\n",
      "74\n",
      "<class 'generator'>\n",
      "75\n",
      "<class 'generator'>\n",
      "76\n",
      "<class 'generator'>\n",
      "77\n",
      "<class 'generator'>\n",
      "78\n",
      "<class 'generator'>\n",
      "79\n",
      "<class 'generator'>\n",
      "80\n",
      "<class 'generator'>\n",
      "81\n",
      "<class 'generator'>\n",
      "82\n",
      "<class 'generator'>\n",
      "83\n",
      "<class 'generator'>\n",
      "84\n",
      "<class 'generator'>\n",
      "85\n",
      "<class 'generator'>\n",
      "86\n",
      "<class 'generator'>\n",
      "87\n",
      "<class 'generator'>\n",
      "88\n",
      "<class 'generator'>\n",
      "89\n",
      "<class 'generator'>\n",
      "90\n",
      "<class 'generator'>\n",
      "91\n",
      "<class 'generator'>\n",
      "92\n",
      "<class 'generator'>\n",
      "93\n",
      "<class 'generator'>\n",
      "94\n",
      "<class 'generator'>\n",
      "95\n",
      "<class 'generator'>\n",
      "96\n",
      "<class 'generator'>\n",
      "97\n",
      "<class 'generator'>\n",
      "98\n",
      "<class 'generator'>\n",
      "99\n",
      "<class 'generator'>\n",
      "100\n",
      "epoch no: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch no: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch: 4, train_loss = 0.0737\n",
      "<class 'generator'>\n",
      "0\n",
      "<class 'generator'>\n",
      "1\n",
      "<class 'generator'>\n",
      "2\n",
      "<class 'generator'>\n",
      "3\n",
      "<class 'generator'>\n",
      "4\n",
      "<class 'generator'>\n",
      "5\n",
      "<class 'generator'>\n",
      "6\n",
      "<class 'generator'>\n",
      "7\n",
      "<class 'generator'>\n",
      "8\n",
      "<class 'generator'>\n",
      "9\n",
      "<class 'generator'>\n",
      "10\n",
      "<class 'generator'>\n",
      "11\n",
      "<class 'generator'>\n",
      "12\n",
      "<class 'generator'>\n",
      "13\n",
      "<class 'generator'>\n",
      "14\n",
      "<class 'generator'>\n",
      "15\n",
      "<class 'generator'>\n",
      "16\n",
      "<class 'generator'>\n",
      "17\n",
      "<class 'generator'>\n",
      "18\n",
      "<class 'generator'>\n",
      "19\n",
      "<class 'generator'>\n",
      "20\n",
      "<class 'generator'>\n",
      "21\n",
      "<class 'generator'>\n",
      "22\n",
      "<class 'generator'>\n",
      "23\n",
      "<class 'generator'>\n",
      "24\n",
      "<class 'generator'>\n",
      "25\n",
      "<class 'generator'>\n",
      "26\n",
      "<class 'generator'>\n",
      "27\n",
      "<class 'generator'>\n",
      "28\n",
      "<class 'generator'>\n",
      "29\n",
      "<class 'generator'>\n",
      "30\n",
      "<class 'generator'>\n",
      "31\n",
      "<class 'generator'>\n",
      "32\n",
      "<class 'generator'>\n",
      "33\n",
      "<class 'generator'>\n",
      "34\n",
      "<class 'generator'>\n",
      "35\n",
      "<class 'generator'>\n",
      "36\n",
      "<class 'generator'>\n",
      "37\n",
      "<class 'generator'>\n",
      "38\n",
      "<class 'generator'>\n",
      "39\n",
      "<class 'generator'>\n",
      "40\n",
      "<class 'generator'>\n",
      "41\n",
      "<class 'generator'>\n",
      "42\n",
      "<class 'generator'>\n",
      "43\n",
      "<class 'generator'>\n",
      "44\n",
      "<class 'generator'>\n",
      "45\n",
      "<class 'generator'>\n",
      "46\n",
      "<class 'generator'>\n",
      "47\n",
      "<class 'generator'>\n",
      "48\n",
      "<class 'generator'>\n",
      "49\n",
      "<class 'generator'>\n",
      "50\n",
      "<class 'generator'>\n",
      "51\n",
      "<class 'generator'>\n",
      "52\n",
      "<class 'generator'>\n",
      "53\n",
      "<class 'generator'>\n",
      "54\n",
      "<class 'generator'>\n",
      "55\n",
      "<class 'generator'>\n",
      "56\n",
      "<class 'generator'>\n",
      "57\n",
      "<class 'generator'>\n",
      "58\n",
      "<class 'generator'>\n",
      "59\n",
      "<class 'generator'>\n",
      "60\n",
      "<class 'generator'>\n",
      "61\n",
      "<class 'generator'>\n",
      "62\n",
      "<class 'generator'>\n",
      "63\n",
      "<class 'generator'>\n",
      "64\n",
      "<class 'generator'>\n",
      "65\n",
      "<class 'generator'>\n",
      "66\n",
      "<class 'generator'>\n",
      "67\n",
      "<class 'generator'>\n",
      "68\n",
      "<class 'generator'>\n",
      "69\n",
      "<class 'generator'>\n",
      "70\n",
      "<class 'generator'>\n",
      "71\n",
      "<class 'generator'>\n",
      "72\n",
      "<class 'generator'>\n",
      "73\n",
      "<class 'generator'>\n",
      "74\n",
      "<class 'generator'>\n",
      "75\n",
      "<class 'generator'>\n",
      "76\n",
      "<class 'generator'>\n",
      "77\n",
      "<class 'generator'>\n",
      "78\n",
      "<class 'generator'>\n",
      "79\n",
      "<class 'generator'>\n",
      "80\n",
      "<class 'generator'>\n",
      "81\n",
      "<class 'generator'>\n",
      "82\n",
      "<class 'generator'>\n",
      "83\n",
      "<class 'generator'>\n",
      "84\n",
      "<class 'generator'>\n",
      "85\n",
      "<class 'generator'>\n",
      "86\n",
      "<class 'generator'>\n",
      "87\n",
      "<class 'generator'>\n",
      "88\n",
      "<class 'generator'>\n",
      "89\n",
      "<class 'generator'>\n",
      "90\n",
      "<class 'generator'>\n",
      "91\n",
      "<class 'generator'>\n",
      "92\n",
      "<class 'generator'>\n",
      "93\n",
      "<class 'generator'>\n",
      "94\n",
      "<class 'generator'>\n",
      "95\n",
      "<class 'generator'>\n",
      "96\n",
      "<class 'generator'>\n",
      "97\n",
      "<class 'generator'>\n",
      "98\n",
      "<class 'generator'>\n",
      "99\n",
      "<class 'generator'>\n",
      "100\n",
      "epoch no: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no: 6\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch: 6, train_loss = 0.0553\n",
      "<class 'generator'>\n",
      "0\n",
      "<class 'generator'>\n",
      "1\n",
      "<class 'generator'>\n",
      "2\n",
      "<class 'generator'>\n",
      "3\n",
      "<class 'generator'>\n",
      "4\n",
      "<class 'generator'>\n",
      "5\n",
      "<class 'generator'>\n",
      "6\n",
      "<class 'generator'>\n",
      "7\n",
      "<class 'generator'>\n",
      "8\n",
      "<class 'generator'>\n",
      "9\n",
      "<class 'generator'>\n",
      "10\n",
      "<class 'generator'>\n",
      "11\n",
      "<class 'generator'>\n",
      "12\n",
      "<class 'generator'>\n",
      "13\n",
      "<class 'generator'>\n",
      "14\n",
      "<class 'generator'>\n",
      "15\n",
      "<class 'generator'>\n",
      "16\n",
      "<class 'generator'>\n",
      "17\n",
      "<class 'generator'>\n",
      "18\n",
      "<class 'generator'>\n",
      "19\n",
      "<class 'generator'>\n",
      "20\n",
      "<class 'generator'>\n",
      "21\n",
      "<class 'generator'>\n",
      "22\n",
      "<class 'generator'>\n",
      "23\n",
      "<class 'generator'>\n",
      "24\n",
      "<class 'generator'>\n",
      "25\n",
      "<class 'generator'>\n",
      "26\n",
      "<class 'generator'>\n",
      "27\n",
      "<class 'generator'>\n",
      "28\n",
      "<class 'generator'>\n",
      "29\n",
      "<class 'generator'>\n",
      "30\n",
      "<class 'generator'>\n",
      "31\n",
      "<class 'generator'>\n",
      "32\n",
      "<class 'generator'>\n",
      "33\n",
      "<class 'generator'>\n",
      "34\n",
      "<class 'generator'>\n",
      "35\n",
      "<class 'generator'>\n",
      "36\n",
      "<class 'generator'>\n",
      "37\n",
      "<class 'generator'>\n",
      "38\n",
      "<class 'generator'>\n",
      "39\n",
      "<class 'generator'>\n",
      "40\n",
      "<class 'generator'>\n",
      "41\n",
      "<class 'generator'>\n",
      "42\n",
      "<class 'generator'>\n",
      "43\n",
      "<class 'generator'>\n",
      "44\n",
      "<class 'generator'>\n",
      "45\n",
      "<class 'generator'>\n",
      "46\n",
      "<class 'generator'>\n",
      "47\n",
      "<class 'generator'>\n",
      "48\n",
      "<class 'generator'>\n",
      "49\n",
      "<class 'generator'>\n",
      "50\n",
      "<class 'generator'>\n",
      "51\n",
      "<class 'generator'>\n",
      "52\n",
      "<class 'generator'>\n",
      "53\n",
      "<class 'generator'>\n",
      "54\n",
      "<class 'generator'>\n",
      "55\n",
      "<class 'generator'>\n",
      "56\n",
      "<class 'generator'>\n",
      "57\n",
      "<class 'generator'>\n",
      "58\n",
      "<class 'generator'>\n",
      "59\n",
      "<class 'generator'>\n",
      "60\n",
      "<class 'generator'>\n",
      "61\n",
      "<class 'generator'>\n",
      "62\n",
      "<class 'generator'>\n",
      "63\n",
      "<class 'generator'>\n",
      "64\n",
      "<class 'generator'>\n",
      "65\n",
      "<class 'generator'>\n",
      "66\n",
      "<class 'generator'>\n",
      "67\n",
      "<class 'generator'>\n",
      "68\n",
      "<class 'generator'>\n",
      "69\n",
      "<class 'generator'>\n",
      "70\n",
      "<class 'generator'>\n",
      "71\n",
      "<class 'generator'>\n",
      "72\n",
      "<class 'generator'>\n",
      "73\n",
      "<class 'generator'>\n",
      "74\n",
      "<class 'generator'>\n",
      "75\n",
      "<class 'generator'>\n",
      "76\n",
      "<class 'generator'>\n",
      "77\n",
      "<class 'generator'>\n",
      "78\n",
      "<class 'generator'>\n",
      "79\n",
      "<class 'generator'>\n",
      "80\n",
      "<class 'generator'>\n",
      "81\n",
      "<class 'generator'>\n",
      "82\n",
      "<class 'generator'>\n",
      "83\n",
      "<class 'generator'>\n",
      "84\n",
      "<class 'generator'>\n",
      "85\n",
      "<class 'generator'>\n",
      "86\n",
      "<class 'generator'>\n",
      "87\n",
      "<class 'generator'>\n",
      "88\n",
      "<class 'generator'>\n",
      "89\n",
      "<class 'generator'>\n",
      "90\n",
      "<class 'generator'>\n",
      "91\n",
      "<class 'generator'>\n",
      "92\n",
      "<class 'generator'>\n",
      "93\n",
      "<class 'generator'>\n",
      "94\n",
      "<class 'generator'>\n",
      "95\n",
      "<class 'generator'>\n",
      "96\n",
      "<class 'generator'>\n",
      "97\n",
      "<class 'generator'>\n",
      "98\n",
      "<class 'generator'>\n",
      "99\n",
      "<class 'generator'>\n",
      "100\n",
      "epoch no: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch no: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch: 8, train_loss = 0.0484\n",
      "<class 'generator'>\n",
      "0\n",
      "<class 'generator'>\n",
      "1\n",
      "<class 'generator'>\n",
      "2\n",
      "<class 'generator'>\n",
      "3\n",
      "<class 'generator'>\n",
      "4\n",
      "<class 'generator'>\n",
      "5\n",
      "<class 'generator'>\n",
      "6\n",
      "<class 'generator'>\n",
      "7\n",
      "<class 'generator'>\n",
      "8\n",
      "<class 'generator'>\n",
      "9\n",
      "<class 'generator'>\n",
      "10\n",
      "<class 'generator'>\n",
      "11\n",
      "<class 'generator'>\n",
      "12\n",
      "<class 'generator'>\n",
      "13\n",
      "<class 'generator'>\n",
      "14\n",
      "<class 'generator'>\n",
      "15\n",
      "<class 'generator'>\n",
      "16\n",
      "<class 'generator'>\n",
      "17\n",
      "<class 'generator'>\n",
      "18\n",
      "<class 'generator'>\n",
      "19\n",
      "<class 'generator'>\n",
      "20\n",
      "<class 'generator'>\n",
      "21\n",
      "<class 'generator'>\n",
      "22\n",
      "<class 'generator'>\n",
      "23\n",
      "<class 'generator'>\n",
      "24\n",
      "<class 'generator'>\n",
      "25\n",
      "<class 'generator'>\n",
      "26\n",
      "<class 'generator'>\n",
      "27\n",
      "<class 'generator'>\n",
      "28\n",
      "<class 'generator'>\n",
      "29\n",
      "<class 'generator'>\n",
      "30\n",
      "<class 'generator'>\n",
      "31\n",
      "<class 'generator'>\n",
      "32\n",
      "<class 'generator'>\n",
      "33\n",
      "<class 'generator'>\n",
      "34\n",
      "<class 'generator'>\n",
      "35\n",
      "<class 'generator'>\n",
      "36\n",
      "<class 'generator'>\n",
      "37\n",
      "<class 'generator'>\n",
      "38\n",
      "<class 'generator'>\n",
      "39\n",
      "<class 'generator'>\n",
      "40\n",
      "<class 'generator'>\n",
      "41\n",
      "<class 'generator'>\n",
      "42\n",
      "<class 'generator'>\n",
      "43\n",
      "<class 'generator'>\n",
      "44\n",
      "<class 'generator'>\n",
      "45\n",
      "<class 'generator'>\n",
      "46\n",
      "<class 'generator'>\n",
      "47\n",
      "<class 'generator'>\n",
      "48\n",
      "<class 'generator'>\n",
      "49\n",
      "<class 'generator'>\n",
      "50\n",
      "<class 'generator'>\n",
      "51\n",
      "<class 'generator'>\n",
      "52\n",
      "<class 'generator'>\n",
      "53\n",
      "<class 'generator'>\n",
      "54\n",
      "<class 'generator'>\n",
      "55\n",
      "<class 'generator'>\n",
      "56\n",
      "<class 'generator'>\n",
      "57\n",
      "<class 'generator'>\n",
      "58\n",
      "<class 'generator'>\n",
      "59\n",
      "<class 'generator'>\n",
      "60\n",
      "<class 'generator'>\n",
      "61\n",
      "<class 'generator'>\n",
      "62\n",
      "<class 'generator'>\n",
      "63\n",
      "<class 'generator'>\n",
      "64\n",
      "<class 'generator'>\n",
      "65\n",
      "<class 'generator'>\n",
      "66\n",
      "<class 'generator'>\n",
      "67\n",
      "<class 'generator'>\n",
      "68\n",
      "<class 'generator'>\n",
      "69\n",
      "<class 'generator'>\n",
      "70\n",
      "<class 'generator'>\n",
      "71\n",
      "<class 'generator'>\n",
      "72\n",
      "<class 'generator'>\n",
      "73\n",
      "<class 'generator'>\n",
      "74\n",
      "<class 'generator'>\n",
      "75\n",
      "<class 'generator'>\n",
      "76\n",
      "<class 'generator'>\n",
      "77\n",
      "<class 'generator'>\n",
      "78\n",
      "<class 'generator'>\n",
      "79\n",
      "<class 'generator'>\n",
      "80\n",
      "<class 'generator'>\n",
      "81\n",
      "<class 'generator'>\n",
      "82\n",
      "<class 'generator'>\n",
      "83\n",
      "<class 'generator'>\n",
      "84\n",
      "<class 'generator'>\n",
      "85\n",
      "<class 'generator'>\n",
      "86\n",
      "<class 'generator'>\n",
      "87\n",
      "<class 'generator'>\n",
      "88\n",
      "<class 'generator'>\n",
      "89\n",
      "<class 'generator'>\n",
      "90\n",
      "<class 'generator'>\n",
      "91\n",
      "<class 'generator'>\n",
      "92\n",
      "<class 'generator'>\n",
      "93\n",
      "<class 'generator'>\n",
      "94\n",
      "<class 'generator'>\n",
      "95\n",
      "<class 'generator'>\n",
      "96\n",
      "<class 'generator'>\n",
      "97\n",
      "<class 'generator'>\n",
      "98\n",
      "<class 'generator'>\n",
      "99\n",
      "<class 'generator'>\n",
      "100\n",
      "epoch no: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch no: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-0ae92af09320cdb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'generator'>\n",
      "<class 'int'>\n",
      "batch no: 0\n",
      "batch no: 1\n",
      "batch no: 2\n",
      "batch no: 3\n",
      "batch no: 4\n",
      "batch no: 5\n",
      "batch no: 6\n",
      "batch no: 7\n",
      "batch no: 8\n",
      "batch no: 9\n",
      "batch no: 10\n",
      "batch no: 11\n",
      "batch no: 12\n",
      "batch no: 13\n",
      "batch no: 14\n",
      "batch no: 15\n",
      "batch no: 16\n",
      "batch no: 17\n",
      "batch no: 18\n",
      "batch no: 19\n",
      "batch no: 20\n",
      "batch no: 21\n",
      "batch no: 22\n",
      "batch no: 23\n",
      "batch no: 24\n",
      "batch no: 25\n",
      "batch no: 26\n",
      "batch no: 27\n",
      "batch no: 28\n",
      "batch no: 29\n",
      "batch no: 30\n",
      "batch no: 31\n",
      "batch no: 32\n",
      "batch no: 33\n",
      "batch no: 34\n",
      "batch no: 35\n",
      "batch no: 36\n",
      "batch no: 37\n",
      "batch no: 38\n",
      "batch no: 39\n",
      "batch no: 40\n",
      "batch no: 41\n",
      "batch no: 42\n",
      "batch no: 43\n",
      "batch no: 44\n",
      "batch no: 45\n",
      "batch no: 46\n",
      "batch no: 47\n",
      "batch no: 48\n",
      "batch no: 49\n",
      "batch no: 50\n",
      "batch no: 51\n",
      "batch no: 52\n",
      "batch no: 53\n",
      "batch no: 54\n",
      "batch no: 55\n",
      "batch no: 56\n",
      "batch no: 57\n",
      "batch no: 58\n",
      "batch no: 59\n",
      "batch no: 60\n",
      "batch no: 61\n",
      "batch no: 62\n",
      "batch no: 63\n",
      "batch no: 64\n",
      "batch no: 65\n",
      "batch no: 66\n",
      "batch no: 67\n",
      "batch no: 68\n",
      "batch no: 69\n",
      "batch no: 70\n",
      "batch no: 71\n",
      "batch no: 72\n",
      "batch no: 73\n",
      "batch no: 74\n",
      "batch no: 75\n",
      "batch no: 76\n",
      "batch no: 77\n",
      "batch no: 78\n",
      "batch no: 79\n",
      "batch no: 80\n",
      "batch no: 81\n",
      "batch no: 82\n",
      "batch no: 83\n",
      "batch no: 84\n",
      "batch no: 85\n",
      "batch no: 86\n",
      "batch no: 87\n",
      "batch no: 88\n",
      "batch no: 89\n",
      "batch no: 90\n",
      "batch no: 91\n",
      "batch no: 92\n",
      "batch no: 93\n",
      "batch no: 94\n",
      "batch no: 95\n",
      "batch no: 96\n",
      "batch no: 97\n",
      "batch no: 98\n",
      "batch no: 99\n",
      "batch no: 100\n",
      "batch no: 101\n",
      "batch no: 102\n",
      "batch no: 103\n",
      "batch no: 104\n",
      "batch no: 105\n",
      "batch no: 106\n",
      "batch no: 107\n",
      "batch no: 108\n",
      "batch no: 109\n",
      "batch no: 110\n",
      "batch no: 111\n",
      "batch no: 112\n",
      "batch no: 113\n",
      "batch no: 114\n",
      "batch no: 115\n",
      "batch no: 116\n",
      "batch no: 117\n",
      "batch no: 118\n",
      "batch no: 119\n",
      "batch no: 120\n",
      "batch no: 121\n",
      "batch no: 122\n",
      "batch no: 123\n",
      "batch no: 124\n",
      "batch no: 125\n",
      "batch no: 126\n",
      "batch no: 127\n",
      "batch no: 128\n",
      "batch no: 129\n",
      "batch no: 130\n",
      "batch no: 131\n",
      "batch no: 132\n",
      "batch no: 133\n",
      "batch no: 134\n",
      "batch no: 135\n",
      "batch no: 136\n",
      "batch no: 137\n",
      "batch no: 138\n",
      "batch no: 139\n",
      "batch no: 140\n",
      "batch no: 141\n",
      "batch no: 142\n",
      "batch no: 143\n",
      "batch no: 144\n",
      "batch no: 145\n",
      "batch no: 146\n",
      "batch no: 147\n",
      "batch no: 148\n",
      "batch no: 149\n",
      "batch no: 150\n",
      "batch no: 151\n",
      "batch no: 152\n",
      "batch no: 153\n",
      "batch no: 154\n",
      "batch no: 155\n",
      "batch no: 156\n",
      "batch no: 157\n",
      "batch no: 158\n",
      "batch no: 159\n",
      "batch no: 160\n",
      "batch no: 161\n",
      "batch no: 162\n",
      "batch no: 163\n",
      "batch no: 164\n",
      "batch no: 165\n",
      "batch no: 166\n",
      "batch no: 167\n",
      "batch no: 168\n",
      "batch no: 169\n",
      "batch no: 170\n",
      "batch no: 171\n",
      "batch no: 172\n",
      "batch no: 173\n",
      "batch no: 174\n",
      "batch no: 175\n",
      "batch no: 176\n",
      "batch no: 177\n",
      "batch no: 178\n",
      "batch no: 179\n",
      "batch no: 180\n",
      "batch no: 181\n",
      "batch no: 182\n",
      "batch no: 183\n",
      "batch no: 184\n",
      "batch no: 185\n",
      "batch no: 186\n",
      "batch no: 187\n",
      "batch no: 188\n",
      "batch no: 189\n",
      "batch no: 190\n",
      "batch no: 191\n",
      "batch no: 192\n",
      "batch no: 193\n",
      "batch no: 194\n",
      "batch no: 195\n",
      "batch no: 196\n",
      "batch no: 197\n",
      "batch no: 198\n",
      "batch no: 199\n",
      "batch no: 200\n",
      "batch no: 201\n",
      "batch no: 202\n",
      "batch no: 203\n",
      "batch no: 204\n",
      "batch no: 205\n",
      "batch no: 206\n",
      "batch no: 207\n",
      "batch no: 208\n",
      "batch no: 209\n",
      "batch no: 210\n",
      "batch no: 211\n",
      "batch no: 212\n",
      "batch no: 213\n",
      "batch no: 214\n",
      "batch no: 215\n",
      "batch no: 216\n",
      "batch no: 217\n",
      "batch no: 218\n",
      "batch no: 219\n",
      "batch no: 220\n",
      "batch no: 221\n",
      "batch no: 222\n",
      "batch no: 223\n",
      "batch no: 224\n",
      "batch no: 225\n",
      "batch no: 226\n",
      "batch no: 227\n",
      "batch no: 228\n",
      "batch no: 229\n",
      "batch no: 230\n",
      "batch no: 231\n",
      "batch no: 232\n",
      "batch no: 233\n",
      "batch no: 234\n",
      "batch no: 235\n",
      "batch no: 236\n",
      "batch no: 237\n",
      "batch no: 238\n",
      "batch no: 239\n",
      "batch no: 240\n",
      "batch no: 241\n",
      "batch no: 242\n",
      "batch no: 243\n",
      "batch no: 244\n",
      "batch no: 245\n",
      "batch no: 246\n",
      "batch no: 247\n",
      "batch no: 248\n",
      "batch no: 249\n",
      "batch no: 250\n",
      "batch no: 251\n",
      "batch no: 252\n",
      "batch no: 253\n",
      "batch no: 254\n",
      "batch no: 255\n",
      "batch no: 256\n",
      "batch no: 257\n",
      "batch no: 258\n",
      "batch no: 259\n",
      "batch no: 260\n",
      "batch no: 261\n",
      "batch no: 262\n",
      "batch no: 263\n",
      "batch no: 264\n",
      "batch no: 265\n",
      "batch no: 266\n",
      "batch no: 267\n",
      "batch no: 268\n",
      "batch no: 269\n",
      "batch no: 270\n",
      "batch no: 271\n",
      "batch no: 272\n",
      "batch no: 273\n",
      "batch no: 274\n",
      "batch no: 275\n",
      "batch no: 276\n",
      "batch no: 277\n",
      "batch no: 278\n",
      "batch no: 279\n",
      "batch no: 280\n",
      "batch no: 281\n",
      "batch no: 282\n",
      "batch no: 283\n",
      "batch no: 284\n",
      "batch no: 285\n",
      "batch no: 286\n",
      "batch no: 287\n",
      "batch no: 288\n",
      "batch no: 289\n",
      "batch no: 290\n",
      "batch no: 291\n",
      "batch no: 292\n",
      "batch no: 293\n",
      "batch no: 294\n",
      "batch no: 295\n",
      "batch no: 296\n",
      "batch no: 297\n",
      "batch no: 298\n",
      "batch no: 299\n",
      "batch no: 300\n",
      "batch no: 301\n",
      "batch no: 302\n",
      "batch no: 303\n",
      "batch no: 304\n",
      "batch no: 305\n",
      "batch no: 306\n",
      "batch no: 307\n",
      "batch no: 308\n",
      "batch no: 309\n",
      "batch no: 310\n",
      "batch no: 311\n",
      "batch no: 312\n",
      "batch no: 313\n",
      "batch no: 314\n",
      "batch no: 315\n",
      "batch no: 316\n",
      "batch no: 317\n",
      "batch no: 318\n",
      "batch no: 319\n",
      "batch no: 320\n",
      "batch no: 321\n",
      "batch no: 322\n",
      "batch no: 323\n",
      "batch no: 324\n",
      "batch no: 325\n",
      "batch no: 326\n",
      "batch no: 327\n",
      "batch no: 328\n",
      "batch no: 329\n",
      "batch no: 330\n",
      "batch no: 331\n",
      "batch no: 332\n",
      "batch no: 333\n",
      "batch no: 334\n",
      "batch no: 335\n",
      "batch no: 336\n",
      "batch no: 337\n",
      "batch no: 338\n",
      "batch no: 339\n",
      "batch no: 340\n",
      "batch no: 341\n",
      "batch no: 342\n",
      "batch no: 343\n",
      "batch no: 344\n",
      "batch no: 345\n",
      "batch no: 346\n",
      "batch no: 347\n",
      "batch no: 348\n",
      "batch no: 349\n",
      "batch no: 350\n",
      "batch no: 351\n",
      "batch no: 352\n",
      "batch no: 353\n",
      "batch no: 354\n",
      "batch no: 355\n",
      "batch no: 356\n",
      "batch no: 357\n",
      "batch no: 358\n",
      "batch no: 359\n",
      "batch no: 360\n",
      "batch no: 361\n",
      "batch no: 362\n",
      "batch no: 363\n",
      "batch no: 364\n",
      "batch no: 365\n",
      "batch no: 366\n",
      "batch no: 367\n",
      "batch no: 368\n",
      "batch no: 369\n",
      "batch no: 370\n",
      "batch no: 371\n",
      "batch no: 372\n",
      "batch no: 373\n",
      "batch no: 374\n",
      "batch no: 375\n",
      "batch no: 376\n",
      "batch no: 377\n",
      "batch no: 378\n",
      "batch no: 379\n",
      "batch no: 380\n",
      "batch no: 381\n",
      "batch no: 382\n",
      "batch no: 383\n",
      "batch no: 384\n",
      "batch no: 385\n",
      "batch no: 386\n",
      "batch no: 387\n",
      "batch no: 388\n",
      "batch no: 389\n",
      "batch no: 390\n",
      "batch no: 391\n",
      "batch no: 392\n",
      "batch no: 393\n",
      "batch no: 394\n",
      "batch no: 395\n",
      "batch no: 396\n",
      "batch no: 397\n",
      "batch no: 398\n",
      "batch no: 399\n",
      "batch no: 400\n",
      "batch no: 401\n",
      "batch no: 402\n",
      "batch no: 403\n",
      "batch no: 404\n",
      "batch no: 405\n",
      "batch no: 406\n",
      "batch no: 407\n",
      "batch no: 408\n",
      "batch no: 409\n",
      "batch no: 410\n",
      "batch no: 411\n",
      "batch no: 412\n",
      "batch no: 413\n",
      "batch no: 414\n",
      "batch no: 415\n",
      "batch no: 416\n",
      "batch no: 417\n",
      "batch no: 418\n",
      "batch no: 419\n",
      "batch no: 420\n",
      "batch no: 421\n",
      "batch no: 422\n",
      "batch no: 423\n",
      "batch no: 424\n",
      "batch no: 425\n",
      "batch no: 426\n",
      "batch no: 427\n",
      "batch no: 428\n",
      "batch no: 429\n",
      "batch no: 430\n",
      "batch no: 431\n",
      "batch no: 432\n",
      "batch no: 433\n",
      "batch no: 434\n",
      "batch no: 435\n",
      "batch no: 436\n",
      "batch no: 437\n",
      "epoch: 10, train_loss = 0.0440\n",
      "<class 'generator'>\n",
      "0\n",
      "<class 'generator'>\n",
      "1\n",
      "<class 'generator'>\n",
      "2\n",
      "<class 'generator'>\n",
      "3\n",
      "<class 'generator'>\n",
      "4\n",
      "<class 'generator'>\n",
      "5\n",
      "<class 'generator'>\n",
      "6\n",
      "<class 'generator'>\n",
      "7\n",
      "<class 'generator'>\n",
      "8\n",
      "<class 'generator'>\n",
      "9\n",
      "<class 'generator'>\n",
      "10\n",
      "<class 'generator'>\n",
      "11\n",
      "<class 'generator'>\n",
      "12\n",
      "<class 'generator'>\n",
      "13\n",
      "<class 'generator'>\n",
      "14\n",
      "<class 'generator'>\n",
      "15\n",
      "<class 'generator'>\n",
      "16\n",
      "<class 'generator'>\n",
      "17\n",
      "<class 'generator'>\n",
      "18\n",
      "<class 'generator'>\n",
      "19\n",
      "<class 'generator'>\n",
      "20\n",
      "<class 'generator'>\n",
      "21\n",
      "<class 'generator'>\n",
      "22\n",
      "<class 'generator'>\n",
      "23\n",
      "<class 'generator'>\n",
      "24\n",
      "<class 'generator'>\n",
      "25\n",
      "<class 'generator'>\n",
      "26\n",
      "<class 'generator'>\n",
      "27\n",
      "<class 'generator'>\n",
      "28\n",
      "<class 'generator'>\n",
      "29\n",
      "<class 'generator'>\n",
      "30\n",
      "<class 'generator'>\n",
      "31\n",
      "<class 'generator'>\n",
      "32\n",
      "<class 'generator'>\n",
      "33\n",
      "<class 'generator'>\n",
      "34\n",
      "<class 'generator'>\n",
      "35\n",
      "<class 'generator'>\n",
      "36\n",
      "<class 'generator'>\n",
      "37\n",
      "<class 'generator'>\n",
      "38\n",
      "<class 'generator'>\n",
      "39\n",
      "<class 'generator'>\n",
      "40\n",
      "<class 'generator'>\n",
      "41\n",
      "<class 'generator'>\n",
      "42\n",
      "<class 'generator'>\n",
      "43\n",
      "<class 'generator'>\n",
      "44\n",
      "<class 'generator'>\n",
      "45\n",
      "<class 'generator'>\n",
      "46\n",
      "<class 'generator'>\n",
      "47\n",
      "<class 'generator'>\n",
      "48\n",
      "<class 'generator'>\n",
      "49\n",
      "<class 'generator'>\n",
      "50\n",
      "<class 'generator'>\n",
      "51\n",
      "<class 'generator'>\n",
      "52\n",
      "<class 'generator'>\n",
      "53\n",
      "<class 'generator'>\n",
      "54\n",
      "<class 'generator'>\n",
      "55\n",
      "<class 'generator'>\n",
      "56\n",
      "<class 'generator'>\n",
      "57\n",
      "<class 'generator'>\n",
      "58\n",
      "<class 'generator'>\n",
      "59\n",
      "<class 'generator'>\n",
      "60\n",
      "<class 'generator'>\n",
      "61\n",
      "<class 'generator'>\n",
      "62\n",
      "<class 'generator'>\n",
      "63\n",
      "<class 'generator'>\n",
      "64\n",
      "<class 'generator'>\n",
      "65\n",
      "<class 'generator'>\n",
      "66\n",
      "<class 'generator'>\n",
      "67\n",
      "<class 'generator'>\n",
      "68\n",
      "<class 'generator'>\n",
      "69\n",
      "<class 'generator'>\n",
      "70\n",
      "<class 'generator'>\n",
      "71\n",
      "<class 'generator'>\n",
      "72\n",
      "<class 'generator'>\n",
      "73\n",
      "<class 'generator'>\n",
      "74\n",
      "<class 'generator'>\n",
      "75\n",
      "<class 'generator'>\n",
      "76\n",
      "<class 'generator'>\n",
      "77\n",
      "<class 'generator'>\n",
      "78\n",
      "<class 'generator'>\n",
      "79\n",
      "<class 'generator'>\n",
      "80\n",
      "<class 'generator'>\n",
      "81\n",
      "<class 'generator'>\n",
      "82\n",
      "<class 'generator'>\n",
      "83\n",
      "<class 'generator'>\n",
      "84\n",
      "<class 'generator'>\n",
      "85\n",
      "<class 'generator'>\n",
      "86\n",
      "<class 'generator'>\n",
      "87\n",
      "<class 'generator'>\n",
      "88\n",
      "<class 'generator'>\n",
      "89\n",
      "<class 'generator'>\n",
      "90\n",
      "<class 'generator'>\n",
      "91\n",
      "<class 'generator'>\n",
      "92\n",
      "<class 'generator'>\n",
      "93\n",
      "<class 'generator'>\n",
      "94\n",
      "<class 'generator'>\n",
      "95\n",
      "<class 'generator'>\n",
      "96\n",
      "<class 'generator'>\n",
      "97\n",
      "<class 'generator'>\n",
      "98\n",
      "<class 'generator'>\n",
      "99\n",
      "<class 'generator'>\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from src.LSTM import RNN\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# CREATE MODEL\n",
    "model = RNN(\n",
    "    embedding_layer=embedding_layer, output_dim=num_classes + 1, hidden_dim_size=256\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "best_accuracy = 0.0\n",
    "print(\"[INFO:] Training classifier...\")\n",
    "\n",
    "epoch_train_loss_history = []\n",
    "epoch_val_loss_history = []\n",
    "running_accuracy = 0.0 \n",
    "running_val_loss = 0.0 \n",
    "total = 0 \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch no: ' + str(epoch + 1))\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    accuracies = []\n",
    "\n",
    "    # shuffle the data\n",
    "    shuffled_train = dataset[\"train\"].shuffle(seed = 1)\n",
    "    shuffled_val = dataset[\"validation\"].shuffle(seed = 1)\n",
    "\n",
    "    if epoch >= 1:\n",
    "        print(type(shuffled_train))\n",
    "        print(type(shuffled_val))\n",
    "        print(type(batches_tokens_train))\n",
    "        print(type(batch_size))\n",
    "        \n",
    "    # generate batches\n",
    "    batches_tokens_train = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "    batches_tags_train = batch(shuffled_train[\"ner_tags\"], batch_size)\n",
    "    batches_tokens_val = batch(shuffled_val[\"tokens\"], batch_size)\n",
    "    batches_tags_val = batch(shuffled_val[\"ner_tags\"], batch_size)\n",
    "\n",
    "    num_batches_train = len(shuffled_train[\"tokens\"])//batch_size\n",
    "    num_batches_val = len(shuffled_val[\"tokens\"])//batch_size\n",
    "\n",
    "    # iterate through the batches\n",
    "    for batch_number in range(num_batches_train):\n",
    "        print('batch no: ' + str(batch_number))\n",
    "    \n",
    "        batch_tokens_train = next(batches_tokens_train)\n",
    "        batch_tags_train = next(batches_tags_train)\n",
    "        batch_tok_idx_train = [tokens_to_idx(sent) for sent in batch_tokens_train]\n",
    "\n",
    "        # compute length of the sentence \n",
    "        batch_max_len_train = max([len(s) for s in batch_tok_idx_train])\n",
    "\n",
    "        # create padding\n",
    "        batch_input_train = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len_train)) # creating an array with a row for each sentence in the batch and a column for each word\n",
    "        batch_labels_train = -1 * np.ones((batch_size, batch_max_len_train)) # Creating a corresponding label\n",
    "\n",
    "        for i in range(batch_size): # looping through each sentence in the batch\n",
    "            tok_idx_train = batch_tok_idx_train[i] #assigning the index to each word\n",
    "            tags_train = batch_tags_train[i]\n",
    "            size = len(tok_idx_train) # number of words in the sentence\n",
    "            #print(len(tok_idx_train))\n",
    "            #print(len(tags_train))\n",
    "\n",
    "            batch_input_train[i][:size] = tok_idx_train # fill in the indexes as inputs\n",
    "            batch_labels_train[i][:size] = tags_train # fill in the tags as labels\n",
    "        \n",
    "        # convert to tensors\n",
    "        batch_input_train, batch_labels_train = torch.LongTensor(batch_input_train), torch.LongTensor(batch_labels_train)\n",
    "\n",
    "        # forward\n",
    "        X_train = batch_input_train\n",
    "        y_train = batch_labels_train\n",
    "        y_train_hat = model(X_train)\n",
    "\n",
    "        # backward\n",
    "        train_loss = model.loss_fn(outputs=y_train_hat, labels=y_train)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        # backpropagation\n",
    "        train_loss.backward()\n",
    "\n",
    "        # take step, reset\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    epoch_train_loss_history.append(np.sum(np.array([val.item() for val in train_loss_history]))/batch_size)\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        # some print to see that it is running\n",
    "        print(f\"epoch: {epoch+1}, train_loss = {train_loss.item():.4f}\")\n",
    "\n",
    "        # see if early stopping is needed\n",
    "        # iterate through the batches\n",
    "        for batch_number_val in range(num_batches_val):\n",
    "            with torch.no_grad(): \n",
    "                model.eval() \n",
    "            #print(batches_tokens_val.shape)\n",
    "            print(type(batches_tokens_val))\n",
    "            print(batch_number_val)\n",
    "            batch_tokens_val = next(batches_tokens_val)\n",
    "            batch_tags_val = next(batches_tags_val)\n",
    "            batch_tok_idx_val = [tokens_to_idx(sent) for sent in batch_tokens_val]\n",
    "\n",
    "            # compute length of the sentence \n",
    "            batch_max_len_val = max([len(s) for s in batch_tok_idx_val])\n",
    "\n",
    "            # create padding\n",
    "            batch_input_val = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len_val)) # creating an array with a row for each sentence in the batch and a column for each word\n",
    "            batch_labels_val = -1 * np.ones((batch_size, batch_max_len_val)) # Creating a corresponding label\n",
    "\n",
    "            for i in range(batch_size): # looping through each sentence in the batch\n",
    "                try:\n",
    "                    tok_idx_val = batch_tok_idx_val[i] #assigning the index to each word\n",
    "                    tags_val = batch_tags_val[i]\n",
    "                    size = len(tok_idx_val) # number of words in the sentence\n",
    "\n",
    "                    batch_input_val[i][:size] = tok_idx_val # fill in the indexes as inputs\n",
    "                    batch_labels_val[i][:size] = tags_val # fill in the tags as labels\n",
    "                \n",
    "                except IndexError:\n",
    "                    break\n",
    "                \n",
    "            # convert to tensors\n",
    "            batch_input_val, batch_labels_val = torch.LongTensor(batch_input_val), torch.LongTensor(batch_labels_val)\n",
    "\n",
    "            # forward\n",
    "            X_val = batch_input_val\n",
    "            y_val = batch_labels_val\n",
    "            y_val_hat = model(X_val)\n",
    "\n",
    "            # backward\n",
    "            y_val_hat = model(X_val) \n",
    "            val_loss = model.loss_fn(outputs=y_val_hat, labels=y_val)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            # the label with the highest value will be our prediction \n",
    "            _, predicted = torch.max(y_val_hat, 1) \n",
    "            running_val_loss += val_loss.item()  \n",
    "            total += y_val.size(0) \n",
    "            predicted = predicted.reshape((y_val.shape[0], y_val.shape[1]))\n",
    "            running_accuracy += (predicted == y_val).sum().item()\n",
    "\n",
    "            # calculate validation loss value \n",
    "            val_loss_value = running_val_loss/len(y_val) \n",
    "                        \n",
    "            # calculate accuracy as the number of correct predictions in the validation batch divided by the total number of predictions done.  \n",
    "            accuracy = (100 * running_accuracy / total) \n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        epoch_acc = np.sum(np.array([val for val in accuracies]))/batch_size\n",
    "        val_loss = np.sum(np.array([val.item() for val in val_loss_history]))/batch_size\n",
    "        epoch_val_loss_history.append(val_loss)  \n",
    "\n",
    "        # save the model if the accuracy is the best \n",
    "        if epoch_acc > best_accuracy: \n",
    "            #saveModel(model) \n",
    "            best_accuracy = epoch_acc\n",
    "\n",
    "    \n",
    "# FORWARD PASS\n",
    "#X = batch_input\n",
    "#y = model(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a loss value is calculated for each batch\n",
    "\n",
    "#loss = model.loss_fn(outputs=y, labels=batch_labels)\n",
    "\n",
    "# etc, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5865368731319904,\n",
       " 0.4674114887602627,\n",
       " 0.42755148850847036,\n",
       " 0.4319482754217461,\n",
       " 0.47347467485815287]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.185462338849902,\n",
       " 2.8742220057174563,\n",
       " 2.2431850160937756,\n",
       " 1.9066979499766603,\n",
       " 1.6644069292815402,\n",
       " 1.4624984838883393,\n",
       " 1.2784701223718002,\n",
       " 1.1048429911024868,\n",
       " 0.9418146846583113,\n",
       " 0.7901578138698824]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_train_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[INFO:] Training classifier...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/work/NLP-AU/nbs/classroom_06.ipynb Cell 50'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://app-237826-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000049vscode-remote?line=25'>26</a>\u001b[0m \u001b[39m# generate batches\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://app-237826-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000049vscode-remote?line=26'>27</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://app-237826-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000049vscode-remote?line=27'>28</a>\u001b[0m batches_tokens \u001b[39m=\u001b[39m batch(shuffled_train[\u001b[39m\"\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m\"\u001b[39;49m], batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-237826-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000049vscode-remote?line=28'>29</a>\u001b[0m batches_tags \u001b[39m=\u001b[39m batch(shuffled_train[\u001b[39m\"\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m\"\u001b[39m], batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-237826-0.cloud.sdu.dk/work/NLP-AU/nbs/classroom_06.ipynb#ch0000049vscode-remote?line=30'>31</a>\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(shuffled_train[\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m32\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from src.LSTM import RNN\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# CREATE MODEL\n",
    "model = RNN(\n",
    "    embedding_layer=embedding_layer, output_dim=num_classes + 1, hidden_dim_size=256\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "best_accuracy = 0.0\n",
    "print(\"[INFO:] Training classifier...\")\n",
    "\n",
    "train_loss_history = []\n",
    "#val_loss_history = []\n",
    "running_accuracy = 0.0 \n",
    "#running_val_loss = 0.0 \n",
    "total = 0 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # shuffle the data\n",
    "    shuffled_train = dataset[\"train\"].shuffle(seed=1)\n",
    "\n",
    "    # generate batches\n",
    "    batch_size = 32\n",
    "    batches_tokens = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "    batches_tags = batch(shuffled_train[\"ner_tags\"], batch_size)\n",
    "\n",
    "    num_batches = len(shuffled_train[\"tokens\"])//32\n",
    "\n",
    "    # iterate through the batches\n",
    "    for batch in range(num_batches):\n",
    "    \n",
    "        batch_tokens = next(batches_tokens)\n",
    "        batch_tags = next(batches_tags)\n",
    "        batch_tok_idx = [tokens_to_idx(sent) for sent in batch_tokens]\n",
    "\n",
    "        # compute length of the sentence \n",
    "        batch_max_len = max([len(s) for s in batch_tok_idx])\n",
    "\n",
    "        # create padding\n",
    "        batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len)) # creating an array with a row for each sentence in the batch and a column for each word\n",
    "        batch_labels = -1 * np.ones((batch_size, batch_max_len)) # Creating a corresponding label\n",
    "\n",
    "        for i in range(batch_size): # looping through each sentence in the batch\n",
    "            tok_idx = batch_tok_idx[i] #assigning the index to each word\n",
    "            tags = batch_tags[i]\n",
    "            size = len(tok_idx) # number of words in the sentence\n",
    "\n",
    "            batch_input[i][:size] = tok_idx # fill in the indexes as inputs\n",
    "            batch_labels[i][:size] = tags # fill in the tags as labels\n",
    "        \n",
    "        # convert to tensors\n",
    "        batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(batch_labels)\n",
    "\n",
    "        # forward\n",
    "        X_train = batch_input\n",
    "        y_train = batch_labels\n",
    "        y_train_hat = model(X_train)\n",
    "\n",
    "        # backward\n",
    "        train_loss = model.loss_fn(outputs=y_train_hat, labels=y_train)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        # backpropagation\n",
    "        train_loss.backward()\n",
    "\n",
    "        # take step, reset\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # some print to see that it is running\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"epoch: {epoch+1}, train_loss = {train_loss.item():.4f}\")\n",
    "    \n",
    "# FORWARD PASS\n",
    "#X = batch_input\n",
    "#y = model(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a loss value is calculated for each batch\n",
    "\n",
    "#loss = model.loss_fn(outputs=y, labels=batch_labels)\n",
    "\n",
    "# etc, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/work/NLP - classrooms/github/NLP-AU/nbs/classroom_06.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://app-235772-0.cloud.sdu.dk/work/NLP%20-%20classrooms/github/NLP-AU/nbs/classroom_06.ipynb#ch0000052vscode-remote?line=8'>9</a>\u001b[0m ax\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-235772-0.cloud.sdu.dk/work/NLP%20-%20classrooms/github/NLP-AU/nbs/classroom_06.ipynb#ch0000052vscode-remote?line=9'>10</a>\u001b[0m ax\u001b[39m.\u001b[39mset_ylabel(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://app-235772-0.cloud.sdu.dk/work/NLP%20-%20classrooms/github/NLP-AU/nbs/classroom_06.ipynb#ch0000052vscode-remote?line=10'>11</a>\u001b[0m ax\u001b[39m.\u001b[39mplot(val_train, label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-235772-0.cloud.sdu.dk/work/NLP%20-%20classrooms/github/NLP-AU/nbs/classroom_06.ipynb#ch0000052vscode-remote?line=11'>12</a>\u001b[0m ax\u001b[39m.\u001b[39mlegend()\n\u001b[1;32m     <a href='vscode-notebook-cell://app-235772-0.cloud.sdu.dk/work/NLP%20-%20classrooms/github/NLP-AU/nbs/classroom_06.ipynb#ch0000052vscode-remote?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39msavefig(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloss_curve_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m classifier \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the training loss history\n",
    "\n",
    "loss_train = [val.item() for val in train_loss_history]\n",
    "val_train = [val.item() for val in val_loss_history]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_train, label = 'train')\n",
    "ax.set_title('Loss history')\n",
    "ax.set_xlabel('Batch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.plot(val_train, label = 'val')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(\"out\", \"loss_curve_\" + classifier + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2.3482, grad_fn=<DivBackward0>),\n",
       " tensor(2.2466, grad_fn=<DivBackward0>),\n",
       " tensor(2.1579, grad_fn=<DivBackward0>),\n",
       " tensor(2.0550, grad_fn=<DivBackward0>),\n",
       " tensor(1.9317, grad_fn=<DivBackward0>),\n",
       " tensor(1.8096, grad_fn=<DivBackward0>),\n",
       " tensor(1.6553, grad_fn=<DivBackward0>),\n",
       " tensor(1.4027, grad_fn=<DivBackward0>),\n",
       " tensor(1.1540, grad_fn=<DivBackward0>),\n",
       " tensor(1.0616, grad_fn=<DivBackward0>),\n",
       " tensor(0.9482, grad_fn=<DivBackward0>),\n",
       " tensor(0.9775, grad_fn=<DivBackward0>),\n",
       " tensor(1.1122, grad_fn=<DivBackward0>),\n",
       " tensor(1.1152, grad_fn=<DivBackward0>),\n",
       " tensor(0.8002, grad_fn=<DivBackward0>),\n",
       " tensor(0.8336, grad_fn=<DivBackward0>),\n",
       " tensor(0.8685, grad_fn=<DivBackward0>),\n",
       " tensor(0.7359, grad_fn=<DivBackward0>),\n",
       " tensor(0.9532, grad_fn=<DivBackward0>),\n",
       " tensor(0.8472, grad_fn=<DivBackward0>),\n",
       " tensor(0.9259, grad_fn=<DivBackward0>),\n",
       " tensor(0.8022, grad_fn=<DivBackward0>),\n",
       " tensor(0.7698, grad_fn=<DivBackward0>),\n",
       " tensor(0.8853, grad_fn=<DivBackward0>),\n",
       " tensor(0.9103, grad_fn=<DivBackward0>),\n",
       " tensor(0.7134, grad_fn=<DivBackward0>),\n",
       " tensor(0.9643, grad_fn=<DivBackward0>),\n",
       " tensor(0.8743, grad_fn=<DivBackward0>),\n",
       " tensor(0.8647, grad_fn=<DivBackward0>),\n",
       " tensor(0.7317, grad_fn=<DivBackward0>),\n",
       " tensor(0.7467, grad_fn=<DivBackward0>),\n",
       " tensor(0.7765, grad_fn=<DivBackward0>),\n",
       " tensor(0.7052, grad_fn=<DivBackward0>),\n",
       " tensor(1.0098, grad_fn=<DivBackward0>),\n",
       " tensor(0.6906, grad_fn=<DivBackward0>),\n",
       " tensor(0.7021, grad_fn=<DivBackward0>),\n",
       " tensor(0.7953, grad_fn=<DivBackward0>),\n",
       " tensor(0.8322, grad_fn=<DivBackward0>),\n",
       " tensor(0.8704, grad_fn=<DivBackward0>),\n",
       " tensor(0.7762, grad_fn=<DivBackward0>),\n",
       " tensor(0.7658, grad_fn=<DivBackward0>),\n",
       " tensor(0.8430, grad_fn=<DivBackward0>),\n",
       " tensor(0.6941, grad_fn=<DivBackward0>),\n",
       " tensor(0.8796, grad_fn=<DivBackward0>),\n",
       " tensor(0.6576, grad_fn=<DivBackward0>),\n",
       " tensor(0.6368, grad_fn=<DivBackward0>),\n",
       " tensor(0.7795, grad_fn=<DivBackward0>),\n",
       " tensor(0.8144, grad_fn=<DivBackward0>),\n",
       " tensor(0.7722, grad_fn=<DivBackward0>),\n",
       " tensor(0.7006, grad_fn=<DivBackward0>),\n",
       " tensor(0.7673, grad_fn=<DivBackward0>),\n",
       " tensor(0.6093, grad_fn=<DivBackward0>),\n",
       " tensor(0.6553, grad_fn=<DivBackward0>),\n",
       " tensor(0.6694, grad_fn=<DivBackward0>),\n",
       " tensor(0.6905, grad_fn=<DivBackward0>),\n",
       " tensor(0.7300, grad_fn=<DivBackward0>),\n",
       " tensor(0.7162, grad_fn=<DivBackward0>),\n",
       " tensor(0.7077, grad_fn=<DivBackward0>),\n",
       " tensor(0.6855, grad_fn=<DivBackward0>),\n",
       " tensor(0.7499, grad_fn=<DivBackward0>),\n",
       " tensor(0.6309, grad_fn=<DivBackward0>),\n",
       " tensor(0.6723, grad_fn=<DivBackward0>),\n",
       " tensor(0.7217, grad_fn=<DivBackward0>),\n",
       " tensor(0.6672, grad_fn=<DivBackward0>),\n",
       " tensor(0.7102, grad_fn=<DivBackward0>),\n",
       " tensor(0.6315, grad_fn=<DivBackward0>),\n",
       " tensor(0.6288, grad_fn=<DivBackward0>),\n",
       " tensor(0.5818, grad_fn=<DivBackward0>),\n",
       " tensor(0.6701, grad_fn=<DivBackward0>),\n",
       " tensor(0.8798, grad_fn=<DivBackward0>),\n",
       " tensor(0.5540, grad_fn=<DivBackward0>),\n",
       " tensor(0.8461, grad_fn=<DivBackward0>),\n",
       " tensor(0.6123, grad_fn=<DivBackward0>),\n",
       " tensor(0.6913, grad_fn=<DivBackward0>),\n",
       " tensor(0.6628, grad_fn=<DivBackward0>),\n",
       " tensor(0.4857, grad_fn=<DivBackward0>),\n",
       " tensor(0.6723, grad_fn=<DivBackward0>),\n",
       " tensor(0.6561, grad_fn=<DivBackward0>),\n",
       " tensor(0.6763, grad_fn=<DivBackward0>),\n",
       " tensor(0.5101, grad_fn=<DivBackward0>),\n",
       " tensor(0.5790, grad_fn=<DivBackward0>),\n",
       " tensor(0.6950, grad_fn=<DivBackward0>),\n",
       " tensor(0.5537, grad_fn=<DivBackward0>),\n",
       " tensor(0.6443, grad_fn=<DivBackward0>),\n",
       " tensor(0.5914, grad_fn=<DivBackward0>),\n",
       " tensor(0.6677, grad_fn=<DivBackward0>),\n",
       " tensor(0.4818, grad_fn=<DivBackward0>),\n",
       " tensor(0.6805, grad_fn=<DivBackward0>),\n",
       " tensor(0.6372, grad_fn=<DivBackward0>),\n",
       " tensor(0.7142, grad_fn=<DivBackward0>),\n",
       " tensor(0.5246, grad_fn=<DivBackward0>),\n",
       " tensor(0.6617, grad_fn=<DivBackward0>),\n",
       " tensor(0.5466, grad_fn=<DivBackward0>),\n",
       " tensor(0.5025, grad_fn=<DivBackward0>),\n",
       " tensor(0.7706, grad_fn=<DivBackward0>),\n",
       " tensor(0.6446, grad_fn=<DivBackward0>),\n",
       " tensor(0.4978, grad_fn=<DivBackward0>),\n",
       " tensor(0.5541, grad_fn=<DivBackward0>),\n",
       " tensor(0.6802, grad_fn=<DivBackward0>),\n",
       " tensor(0.6045, grad_fn=<DivBackward0>),\n",
       " tensor(0.5654, grad_fn=<DivBackward0>),\n",
       " tensor(0.6113, grad_fn=<DivBackward0>),\n",
       " tensor(0.6937, grad_fn=<DivBackward0>),\n",
       " tensor(0.5475, grad_fn=<DivBackward0>),\n",
       " tensor(0.4441, grad_fn=<DivBackward0>),\n",
       " tensor(0.4823, grad_fn=<DivBackward0>),\n",
       " tensor(0.6396, grad_fn=<DivBackward0>),\n",
       " tensor(0.6909, grad_fn=<DivBackward0>),\n",
       " tensor(0.6473, grad_fn=<DivBackward0>),\n",
       " tensor(0.6306, grad_fn=<DivBackward0>),\n",
       " tensor(0.5947, grad_fn=<DivBackward0>),\n",
       " tensor(0.5229, grad_fn=<DivBackward0>),\n",
       " tensor(0.5411, grad_fn=<DivBackward0>),\n",
       " tensor(0.4657, grad_fn=<DivBackward0>),\n",
       " tensor(0.4866, grad_fn=<DivBackward0>),\n",
       " tensor(0.4827, grad_fn=<DivBackward0>),\n",
       " tensor(0.3661, grad_fn=<DivBackward0>),\n",
       " tensor(0.4284, grad_fn=<DivBackward0>),\n",
       " tensor(0.6496, grad_fn=<DivBackward0>),\n",
       " tensor(0.5795, grad_fn=<DivBackward0>),\n",
       " tensor(0.4924, grad_fn=<DivBackward0>),\n",
       " tensor(0.4812, grad_fn=<DivBackward0>),\n",
       " tensor(0.5662, grad_fn=<DivBackward0>),\n",
       " tensor(0.4494, grad_fn=<DivBackward0>),\n",
       " tensor(0.5514, grad_fn=<DivBackward0>),\n",
       " tensor(0.4187, grad_fn=<DivBackward0>),\n",
       " tensor(0.5289, grad_fn=<DivBackward0>),\n",
       " tensor(0.4961, grad_fn=<DivBackward0>),\n",
       " tensor(0.5366, grad_fn=<DivBackward0>),\n",
       " tensor(0.5393, grad_fn=<DivBackward0>),\n",
       " tensor(0.4768, grad_fn=<DivBackward0>),\n",
       " tensor(0.5250, grad_fn=<DivBackward0>),\n",
       " tensor(0.5631, grad_fn=<DivBackward0>),\n",
       " tensor(0.5772, grad_fn=<DivBackward0>),\n",
       " tensor(0.5577, grad_fn=<DivBackward0>),\n",
       " tensor(0.4019, grad_fn=<DivBackward0>),\n",
       " tensor(0.4554, grad_fn=<DivBackward0>),\n",
       " tensor(0.3915, grad_fn=<DivBackward0>),\n",
       " tensor(0.4661, grad_fn=<DivBackward0>),\n",
       " tensor(0.4551, grad_fn=<DivBackward0>),\n",
       " tensor(0.5433, grad_fn=<DivBackward0>),\n",
       " tensor(0.5237, grad_fn=<DivBackward0>),\n",
       " tensor(0.3945, grad_fn=<DivBackward0>),\n",
       " tensor(0.3479, grad_fn=<DivBackward0>),\n",
       " tensor(0.4261, grad_fn=<DivBackward0>),\n",
       " tensor(0.3855, grad_fn=<DivBackward0>),\n",
       " tensor(0.4703, grad_fn=<DivBackward0>),\n",
       " tensor(0.4332, grad_fn=<DivBackward0>),\n",
       " tensor(0.4689, grad_fn=<DivBackward0>),\n",
       " tensor(0.5067, grad_fn=<DivBackward0>),\n",
       " tensor(0.4467, grad_fn=<DivBackward0>),\n",
       " tensor(0.3413, grad_fn=<DivBackward0>),\n",
       " tensor(0.4258, grad_fn=<DivBackward0>),\n",
       " tensor(0.5049, grad_fn=<DivBackward0>),\n",
       " tensor(0.3637, grad_fn=<DivBackward0>),\n",
       " tensor(0.4572, grad_fn=<DivBackward0>),\n",
       " tensor(0.4793, grad_fn=<DivBackward0>),\n",
       " tensor(0.4763, grad_fn=<DivBackward0>),\n",
       " tensor(0.4108, grad_fn=<DivBackward0>),\n",
       " tensor(0.4250, grad_fn=<DivBackward0>),\n",
       " tensor(0.3765, grad_fn=<DivBackward0>),\n",
       " tensor(0.4512, grad_fn=<DivBackward0>),\n",
       " tensor(0.4486, grad_fn=<DivBackward0>),\n",
       " tensor(0.4917, grad_fn=<DivBackward0>),\n",
       " tensor(0.4820, grad_fn=<DivBackward0>),\n",
       " tensor(0.3125, grad_fn=<DivBackward0>),\n",
       " tensor(0.4118, grad_fn=<DivBackward0>),\n",
       " tensor(0.3388, grad_fn=<DivBackward0>),\n",
       " tensor(0.4236, grad_fn=<DivBackward0>),\n",
       " tensor(0.3763, grad_fn=<DivBackward0>),\n",
       " tensor(0.3408, grad_fn=<DivBackward0>),\n",
       " tensor(0.3165, grad_fn=<DivBackward0>),\n",
       " tensor(0.3854, grad_fn=<DivBackward0>),\n",
       " tensor(0.3995, grad_fn=<DivBackward0>),\n",
       " tensor(0.4278, grad_fn=<DivBackward0>),\n",
       " tensor(0.3988, grad_fn=<DivBackward0>),\n",
       " tensor(0.3477, grad_fn=<DivBackward0>),\n",
       " tensor(0.3668, grad_fn=<DivBackward0>),\n",
       " tensor(0.3748, grad_fn=<DivBackward0>),\n",
       " tensor(0.4267, grad_fn=<DivBackward0>),\n",
       " tensor(0.4218, grad_fn=<DivBackward0>),\n",
       " tensor(0.4454, grad_fn=<DivBackward0>),\n",
       " tensor(0.4530, grad_fn=<DivBackward0>),\n",
       " tensor(0.3336, grad_fn=<DivBackward0>),\n",
       " tensor(0.3701, grad_fn=<DivBackward0>),\n",
       " tensor(0.3833, grad_fn=<DivBackward0>),\n",
       " tensor(0.3515, grad_fn=<DivBackward0>),\n",
       " tensor(0.3367, grad_fn=<DivBackward0>),\n",
       " tensor(0.5680, grad_fn=<DivBackward0>),\n",
       " tensor(0.4128, grad_fn=<DivBackward0>),\n",
       " tensor(0.3825, grad_fn=<DivBackward0>),\n",
       " tensor(0.4058, grad_fn=<DivBackward0>),\n",
       " tensor(0.2830, grad_fn=<DivBackward0>),\n",
       " tensor(0.3262, grad_fn=<DivBackward0>),\n",
       " tensor(0.5072, grad_fn=<DivBackward0>),\n",
       " tensor(0.3887, grad_fn=<DivBackward0>),\n",
       " tensor(0.4540, grad_fn=<DivBackward0>),\n",
       " tensor(0.3679, grad_fn=<DivBackward0>),\n",
       " tensor(0.2916, grad_fn=<DivBackward0>),\n",
       " tensor(0.4036, grad_fn=<DivBackward0>),\n",
       " tensor(0.4421, grad_fn=<DivBackward0>),\n",
       " tensor(0.4086, grad_fn=<DivBackward0>),\n",
       " tensor(0.4455, grad_fn=<DivBackward0>),\n",
       " tensor(0.4375, grad_fn=<DivBackward0>),\n",
       " tensor(0.3587, grad_fn=<DivBackward0>),\n",
       " tensor(0.4511, grad_fn=<DivBackward0>),\n",
       " tensor(0.3437, grad_fn=<DivBackward0>),\n",
       " tensor(0.4562, grad_fn=<DivBackward0>),\n",
       " tensor(0.3976, grad_fn=<DivBackward0>),\n",
       " tensor(0.3748, grad_fn=<DivBackward0>),\n",
       " tensor(0.4136, grad_fn=<DivBackward0>),\n",
       " tensor(0.2790, grad_fn=<DivBackward0>),\n",
       " tensor(0.3815, grad_fn=<DivBackward0>),\n",
       " tensor(0.4345, grad_fn=<DivBackward0>),\n",
       " tensor(0.3785, grad_fn=<DivBackward0>),\n",
       " tensor(0.4127, grad_fn=<DivBackward0>),\n",
       " tensor(0.4894, grad_fn=<DivBackward0>),\n",
       " tensor(0.4756, grad_fn=<DivBackward0>),\n",
       " tensor(0.2984, grad_fn=<DivBackward0>),\n",
       " tensor(0.4850, grad_fn=<DivBackward0>),\n",
       " tensor(0.3578, grad_fn=<DivBackward0>),\n",
       " tensor(0.4154, grad_fn=<DivBackward0>),\n",
       " tensor(0.3119, grad_fn=<DivBackward0>),\n",
       " tensor(0.3149, grad_fn=<DivBackward0>),\n",
       " tensor(0.3236, grad_fn=<DivBackward0>),\n",
       " tensor(0.3420, grad_fn=<DivBackward0>),\n",
       " tensor(0.3778, grad_fn=<DivBackward0>),\n",
       " tensor(0.3703, grad_fn=<DivBackward0>),\n",
       " tensor(0.2187, grad_fn=<DivBackward0>),\n",
       " tensor(0.3208, grad_fn=<DivBackward0>),\n",
       " tensor(0.3606, grad_fn=<DivBackward0>),\n",
       " tensor(0.2646, grad_fn=<DivBackward0>),\n",
       " tensor(0.4477, grad_fn=<DivBackward0>),\n",
       " tensor(0.3365, grad_fn=<DivBackward0>),\n",
       " tensor(0.2844, grad_fn=<DivBackward0>),\n",
       " tensor(0.3292, grad_fn=<DivBackward0>),\n",
       " tensor(0.3362, grad_fn=<DivBackward0>),\n",
       " tensor(0.3417, grad_fn=<DivBackward0>),\n",
       " tensor(0.3329, grad_fn=<DivBackward0>),\n",
       " tensor(0.2686, grad_fn=<DivBackward0>),\n",
       " tensor(0.3261, grad_fn=<DivBackward0>),\n",
       " tensor(0.3154, grad_fn=<DivBackward0>),\n",
       " tensor(0.3101, grad_fn=<DivBackward0>),\n",
       " tensor(0.4572, grad_fn=<DivBackward0>),\n",
       " tensor(0.2957, grad_fn=<DivBackward0>),\n",
       " tensor(0.2524, grad_fn=<DivBackward0>),\n",
       " tensor(0.3517, grad_fn=<DivBackward0>),\n",
       " tensor(0.4108, grad_fn=<DivBackward0>),\n",
       " tensor(0.2688, grad_fn=<DivBackward0>),\n",
       " tensor(0.4350, grad_fn=<DivBackward0>),\n",
       " tensor(0.4562, grad_fn=<DivBackward0>),\n",
       " tensor(0.3427, grad_fn=<DivBackward0>),\n",
       " tensor(0.3501, grad_fn=<DivBackward0>),\n",
       " tensor(0.4087, grad_fn=<DivBackward0>),\n",
       " tensor(0.2566, grad_fn=<DivBackward0>),\n",
       " tensor(0.2606, grad_fn=<DivBackward0>),\n",
       " tensor(0.2060, grad_fn=<DivBackward0>),\n",
       " tensor(0.3376, grad_fn=<DivBackward0>),\n",
       " tensor(0.4444, grad_fn=<DivBackward0>),\n",
       " tensor(0.3231, grad_fn=<DivBackward0>),\n",
       " tensor(0.3195, grad_fn=<DivBackward0>),\n",
       " tensor(0.3641, grad_fn=<DivBackward0>),\n",
       " tensor(0.3263, grad_fn=<DivBackward0>),\n",
       " tensor(0.4663, grad_fn=<DivBackward0>),\n",
       " tensor(0.3064, grad_fn=<DivBackward0>),\n",
       " tensor(0.3008, grad_fn=<DivBackward0>),\n",
       " tensor(0.2962, grad_fn=<DivBackward0>),\n",
       " tensor(0.3147, grad_fn=<DivBackward0>),\n",
       " tensor(0.2917, grad_fn=<DivBackward0>),\n",
       " tensor(0.3534, grad_fn=<DivBackward0>),\n",
       " tensor(0.2701, grad_fn=<DivBackward0>),\n",
       " tensor(0.2255, grad_fn=<DivBackward0>),\n",
       " tensor(0.3290, grad_fn=<DivBackward0>),\n",
       " tensor(0.3112, grad_fn=<DivBackward0>),\n",
       " tensor(0.3397, grad_fn=<DivBackward0>),\n",
       " tensor(0.2772, grad_fn=<DivBackward0>),\n",
       " tensor(0.2269, grad_fn=<DivBackward0>),\n",
       " tensor(0.2982, grad_fn=<DivBackward0>),\n",
       " tensor(0.3954, grad_fn=<DivBackward0>),\n",
       " tensor(0.3608, grad_fn=<DivBackward0>),\n",
       " tensor(0.2188, grad_fn=<DivBackward0>),\n",
       " tensor(0.3017, grad_fn=<DivBackward0>),\n",
       " tensor(0.2961, grad_fn=<DivBackward0>),\n",
       " tensor(0.3182, grad_fn=<DivBackward0>),\n",
       " tensor(0.3949, grad_fn=<DivBackward0>),\n",
       " tensor(0.2790, grad_fn=<DivBackward0>),\n",
       " tensor(0.3294, grad_fn=<DivBackward0>),\n",
       " tensor(0.2377, grad_fn=<DivBackward0>),\n",
       " tensor(0.2462, grad_fn=<DivBackward0>),\n",
       " tensor(0.3417, grad_fn=<DivBackward0>),\n",
       " tensor(0.2790, grad_fn=<DivBackward0>),\n",
       " tensor(0.3327, grad_fn=<DivBackward0>),\n",
       " tensor(0.2023, grad_fn=<DivBackward0>),\n",
       " tensor(0.2303, grad_fn=<DivBackward0>),\n",
       " tensor(0.3569, grad_fn=<DivBackward0>),\n",
       " tensor(0.2418, grad_fn=<DivBackward0>),\n",
       " tensor(0.3276, grad_fn=<DivBackward0>),\n",
       " tensor(0.3009, grad_fn=<DivBackward0>),\n",
       " tensor(0.3069, grad_fn=<DivBackward0>),\n",
       " tensor(0.3369, grad_fn=<DivBackward0>),\n",
       " tensor(0.2906, grad_fn=<DivBackward0>),\n",
       " tensor(0.2341, grad_fn=<DivBackward0>),\n",
       " tensor(0.3020, grad_fn=<DivBackward0>),\n",
       " tensor(0.3232, grad_fn=<DivBackward0>),\n",
       " tensor(0.2515, grad_fn=<DivBackward0>),\n",
       " tensor(0.2728, grad_fn=<DivBackward0>),\n",
       " tensor(0.2534, grad_fn=<DivBackward0>),\n",
       " tensor(0.3422, grad_fn=<DivBackward0>),\n",
       " tensor(0.2887, grad_fn=<DivBackward0>),\n",
       " tensor(0.2548, grad_fn=<DivBackward0>),\n",
       " tensor(0.2850, grad_fn=<DivBackward0>),\n",
       " tensor(0.3642, grad_fn=<DivBackward0>),\n",
       " tensor(0.4395, grad_fn=<DivBackward0>),\n",
       " tensor(0.2998, grad_fn=<DivBackward0>),\n",
       " tensor(0.2300, grad_fn=<DivBackward0>),\n",
       " tensor(0.2872, grad_fn=<DivBackward0>),\n",
       " tensor(0.2449, grad_fn=<DivBackward0>),\n",
       " tensor(0.2938, grad_fn=<DivBackward0>),\n",
       " tensor(0.2375, grad_fn=<DivBackward0>),\n",
       " tensor(0.2909, grad_fn=<DivBackward0>),\n",
       " tensor(0.2090, grad_fn=<DivBackward0>),\n",
       " tensor(0.3789, grad_fn=<DivBackward0>),\n",
       " tensor(0.3412, grad_fn=<DivBackward0>),\n",
       " tensor(0.2863, grad_fn=<DivBackward0>),\n",
       " tensor(0.2645, grad_fn=<DivBackward0>),\n",
       " tensor(0.2814, grad_fn=<DivBackward0>),\n",
       " tensor(0.3583, grad_fn=<DivBackward0>),\n",
       " tensor(0.3206, grad_fn=<DivBackward0>),\n",
       " tensor(0.2636, grad_fn=<DivBackward0>),\n",
       " tensor(0.3742, grad_fn=<DivBackward0>),\n",
       " tensor(0.2788, grad_fn=<DivBackward0>),\n",
       " tensor(0.3010, grad_fn=<DivBackward0>),\n",
       " tensor(0.2134, grad_fn=<DivBackward0>),\n",
       " tensor(0.3012, grad_fn=<DivBackward0>),\n",
       " tensor(0.3571, grad_fn=<DivBackward0>),\n",
       " tensor(0.3283, grad_fn=<DivBackward0>),\n",
       " tensor(0.2102, grad_fn=<DivBackward0>),\n",
       " tensor(0.2324, grad_fn=<DivBackward0>),\n",
       " tensor(0.2666, grad_fn=<DivBackward0>),\n",
       " tensor(0.2485, grad_fn=<DivBackward0>),\n",
       " tensor(0.3293, grad_fn=<DivBackward0>),\n",
       " tensor(0.3240, grad_fn=<DivBackward0>),\n",
       " tensor(0.2937, grad_fn=<DivBackward0>),\n",
       " tensor(0.1809, grad_fn=<DivBackward0>),\n",
       " tensor(0.2487, grad_fn=<DivBackward0>),\n",
       " tensor(0.2282, grad_fn=<DivBackward0>),\n",
       " tensor(0.3068, grad_fn=<DivBackward0>),\n",
       " tensor(0.2198, grad_fn=<DivBackward0>),\n",
       " tensor(0.1748, grad_fn=<DivBackward0>),\n",
       " tensor(0.2808, grad_fn=<DivBackward0>),\n",
       " tensor(0.2363, grad_fn=<DivBackward0>),\n",
       " tensor(0.2615, grad_fn=<DivBackward0>),\n",
       " tensor(0.3619, grad_fn=<DivBackward0>),\n",
       " tensor(0.2162, grad_fn=<DivBackward0>),\n",
       " tensor(0.2355, grad_fn=<DivBackward0>),\n",
       " tensor(0.2880, grad_fn=<DivBackward0>),\n",
       " tensor(0.3677, grad_fn=<DivBackward0>),\n",
       " tensor(0.1476, grad_fn=<DivBackward0>),\n",
       " tensor(0.4402, grad_fn=<DivBackward0>),\n",
       " tensor(0.2584, grad_fn=<DivBackward0>),\n",
       " tensor(0.2251, grad_fn=<DivBackward0>),\n",
       " tensor(0.2236, grad_fn=<DivBackward0>),\n",
       " tensor(0.2815, grad_fn=<DivBackward0>),\n",
       " tensor(0.2176, grad_fn=<DivBackward0>),\n",
       " tensor(0.3551, grad_fn=<DivBackward0>),\n",
       " tensor(0.2729, grad_fn=<DivBackward0>),\n",
       " tensor(0.2227, grad_fn=<DivBackward0>),\n",
       " tensor(0.2179, grad_fn=<DivBackward0>),\n",
       " tensor(0.2657, grad_fn=<DivBackward0>),\n",
       " tensor(0.3427, grad_fn=<DivBackward0>),\n",
       " tensor(0.2166, grad_fn=<DivBackward0>),\n",
       " tensor(0.2746, grad_fn=<DivBackward0>),\n",
       " tensor(0.2503, grad_fn=<DivBackward0>),\n",
       " tensor(0.2940, grad_fn=<DivBackward0>),\n",
       " tensor(0.2367, grad_fn=<DivBackward0>),\n",
       " tensor(0.3203, grad_fn=<DivBackward0>),\n",
       " tensor(0.2441, grad_fn=<DivBackward0>),\n",
       " tensor(0.2588, grad_fn=<DivBackward0>),\n",
       " tensor(0.2452, grad_fn=<DivBackward0>),\n",
       " tensor(0.2822, grad_fn=<DivBackward0>),\n",
       " tensor(0.2892, grad_fn=<DivBackward0>),\n",
       " tensor(0.2684, grad_fn=<DivBackward0>),\n",
       " tensor(0.2113, grad_fn=<DivBackward0>),\n",
       " tensor(0.1914, grad_fn=<DivBackward0>),\n",
       " tensor(0.2378, grad_fn=<DivBackward0>),\n",
       " tensor(0.2415, grad_fn=<DivBackward0>),\n",
       " tensor(0.2618, grad_fn=<DivBackward0>),\n",
       " tensor(0.3635, grad_fn=<DivBackward0>),\n",
       " tensor(0.2832, grad_fn=<DivBackward0>),\n",
       " tensor(0.2215, grad_fn=<DivBackward0>),\n",
       " tensor(0.3386, grad_fn=<DivBackward0>),\n",
       " tensor(0.2561, grad_fn=<DivBackward0>),\n",
       " tensor(0.1857, grad_fn=<DivBackward0>),\n",
       " tensor(0.2397, grad_fn=<DivBackward0>),\n",
       " tensor(0.3163, grad_fn=<DivBackward0>),\n",
       " tensor(0.3014, grad_fn=<DivBackward0>),\n",
       " tensor(0.2696, grad_fn=<DivBackward0>),\n",
       " tensor(0.3841, grad_fn=<DivBackward0>),\n",
       " tensor(0.3146, grad_fn=<DivBackward0>),\n",
       " tensor(0.2498, grad_fn=<DivBackward0>),\n",
       " tensor(0.2721, grad_fn=<DivBackward0>),\n",
       " tensor(0.2766, grad_fn=<DivBackward0>),\n",
       " tensor(0.2638, grad_fn=<DivBackward0>),\n",
       " tensor(0.2277, grad_fn=<DivBackward0>),\n",
       " tensor(0.2420, grad_fn=<DivBackward0>),\n",
       " tensor(0.3069, grad_fn=<DivBackward0>),\n",
       " tensor(0.2147, grad_fn=<DivBackward0>),\n",
       " tensor(0.2800, grad_fn=<DivBackward0>),\n",
       " tensor(0.3195, grad_fn=<DivBackward0>),\n",
       " tensor(0.1510, grad_fn=<DivBackward0>),\n",
       " tensor(0.2484, grad_fn=<DivBackward0>),\n",
       " tensor(0.3398, grad_fn=<DivBackward0>),\n",
       " tensor(0.2068, grad_fn=<DivBackward0>),\n",
       " tensor(0.4055, grad_fn=<DivBackward0>),\n",
       " tensor(0.2898, grad_fn=<DivBackward0>),\n",
       " tensor(0.3808, grad_fn=<DivBackward0>),\n",
       " tensor(0.2108, grad_fn=<DivBackward0>),\n",
       " tensor(0.2302, grad_fn=<DivBackward0>),\n",
       " tensor(0.2503, grad_fn=<DivBackward0>),\n",
       " tensor(0.1990, grad_fn=<DivBackward0>),\n",
       " tensor(0.1570, grad_fn=<DivBackward0>),\n",
       " tensor(0.3170, grad_fn=<DivBackward0>),\n",
       " tensor(0.2208, grad_fn=<DivBackward0>),\n",
       " tensor(0.3588, grad_fn=<DivBackward0>),\n",
       " tensor(0.2872, grad_fn=<DivBackward0>),\n",
       " tensor(0.2793, grad_fn=<DivBackward0>),\n",
       " tensor(0.2974, grad_fn=<DivBackward0>),\n",
       " tensor(0.2235, grad_fn=<DivBackward0>),\n",
       " tensor(0.2551, grad_fn=<DivBackward0>),\n",
       " tensor(0.2234, grad_fn=<DivBackward0>),\n",
       " tensor(0.2903, grad_fn=<DivBackward0>),\n",
       " tensor(0.2692, grad_fn=<DivBackward0>),\n",
       " tensor(0.4012, grad_fn=<DivBackward0>),\n",
       " tensor(0.2887, grad_fn=<DivBackward0>),\n",
       " tensor(0.2546, grad_fn=<DivBackward0>),\n",
       " tensor(0.2582, grad_fn=<DivBackward0>),\n",
       " tensor(0.2327, grad_fn=<DivBackward0>),\n",
       " tensor(0.1848, grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(shuffled_train[\"ner_tags\"]))\n",
    "print(type(shuffled_train[\"ner_tags\"][0]))\n",
    "shuffled_train[\"ner_tags\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an LSTM with ```pytorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file [LSTM.py](../src/LSTM.py), I've aready created an LSTM for you using ```pytorch```. Take some time to read through the code and make sure you understand how it's built up.\n",
    "\n",
    "Some questions for you to discuss in groups:\n",
    "\n",
    "- How is an LSTM layer created using ```pytorch```? How does the code compare to the classifier code you wrote last week?\n",
    "- What's going on with that weird bit that says ```@staticmethod```?\n",
    "  - [This might help](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "- On the forward pass, we use ```log_softmax()``` to make output predictions. What is this, and how does it relate to the output from the sigmoid function that we used in the document classification?\n",
    "  * Answer: Log Softmax is advantageous over softmax for numerical stability, optimisation and heavy penalisation for highly incorrect class\n",
    "- How would we make this LSTM model *bidirectional* - i.e. make it a Bi-LSTM? \n",
    "  - Hint: Check the documentation for the LSTM layer on the ```pytorch``` website.\n",
    "  * Answer: to apply the LSTM layer for both the words before and after the one we for example want to predict the ner tag for we simply set bi-directional = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
